{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "Welcome to this week's programming assignment! Up until now, you've always used Numpy to build neural networks, but this week you'll explore a deep learning framework that allows you to build neural networks more easily. Machine learning frameworks like TensorFlow, PaddlePaddle, Torch, Caffe, Keras, and many others can speed up your machine learning development significantly. PyTorch has made significant improvements over its predecessor, some of which you'll encounter and implement here!\n",
    "\n",
    "Programming frameworks like PyTorch not only cut down on time spent coding, but can also perform optimizations that speed up the code itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1- Packages](#1)\n",
    "    - [1.1 - Checking TensorFlow Version](#1-1)\n",
    "- [2 - Basic Optimization with GradientTape](#2)\n",
    "    - [2.1 - Linear Function](#2-1)\n",
    "        - [Exercise 1 - linear_function](#ex-1)\n",
    "    - [2.2 - Computing the Sigmoid](#2-2)\n",
    "        - [Exercise 2 - sigmoid](#ex-2)\n",
    "    - [2.3 - Using One Hot Encodings](#2-3)\n",
    "        - [Exercise 3 - one_hot_matrix](#ex-3)\n",
    "    - [2.4 - Initialize the Parameters](#2-4)\n",
    "        - [Exercise 4 - initialize_parameters](#ex-4)\n",
    "- [3 - Building Your First Neural Network in TensorFlow](#3)\n",
    "    - [3.1 - Implement Forward Propagation](#3-1)\n",
    "        - [Exercise 5 - forward_propagation](#ex-5)\n",
    "    - [3.2 Compute the Cost](#3-2)\n",
    "        - [Exercise 6 - compute_cost](#ex-6)\n",
    "    - [3.3 - Train the Model](#3-3)\n",
    "- [4 - Bibliography](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - Checking PyTorch Version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = h5py.File('./datasets/train_signs.h5', \"r\")\n",
    "test_dataset = h5py.File('./datasets/test_signs.h5', \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(train_dataset['train_set_x'])\n",
    "y_train = np.array(train_dataset['train_set_y'])\n",
    "\n",
    "x_test = np.array(test_dataset['test_set_x'])\n",
    "y_test = np.array(test_dataset['test_set_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Dataset & DataLoader\n",
    "데이터 샘플을 처리하는 코드는 지저분(messy)하고 유지보수가 어려울 수 있습니다; 더 나은 가독성(readability)과 모듈성(modularity)을 위해 데이터셋 코드를 모델 학습 코드로부터 분리하는 것이 이상적입니다. PyTorch는 torch.utils.data.DataLoader 와 torch.utils.data.Dataset 의 두 가지 데이터 기본 요소를 제공하여 미리 준비해둔(pre-loaded) 데이터셋 뿐만 아니라 가지고 있는 데이터를 사용할 수 있도록 합니다. Dataset 은 샘플과 정답(label)을 저장하고, DataLoader 는 Dataset 을 샘플에 쉽게 접근할 수 있도록 순회 가능한 객체(iterable)로 감쌉니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Dataset\n",
    "\n",
    "사용자 정의 Dataset 클래스는 반드시 3개 함수를 구현해야 합니다: \\_\\_init\\_\\_, \\_\\_len\\_\\_, and \\_\\_getitem\\_\\_. \n",
    "\n",
    "`__init__`  \n",
    "\\_\\_init\\_\\_ 함수는 Dataset 객체가 생성(instantiate)될 때 한 번만 실행됩니다. \n",
    "\n",
    "`__len__`  \n",
    "\\_\\_len\\_\\_ 함수는 데이터셋의 샘플 개수를 반환합니다.\n",
    "\n",
    "`__getitem__`  \n",
    "\\_\\_getitem\\_\\_ 함수는 주어진 인덱스 idx 에 해당하는 샘플을 데이터셋에서 불러오고 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # normalize(/255.0)\n",
    "        # flatten(use reshape(-1))\n",
    "        normalized_x = self.x[idx] / 255\n",
    "        # YOUR CODE STARTS HERE\n",
    "        reshaped_normalized_x = normalized_x.reshape(-1)\n",
    "        # YOUR CODE ENDS HERE\n",
    "        return {\n",
    "            'x' : reshaped_normalized_x,\n",
    "            'y' : self.y[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_custom_dataset = Custom_Dataset(x_train, y_train)\n",
    "test_custom_dataset = Custom_Dataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(train_custom_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('__len__ :', len(train_custom_dataset))\n",
    "print('__len__ :', train_custom_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('__getitem__ :', train_custom_dataset[0])\n",
    "print('__getitem__ :', train_custom_dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch DataLoader\n",
    "`Dataset` 은 데이터셋의 특징(feature)을 가져오고 하나의 샘플에 정답(label)을 지정하는 일을 한 번에 합니다. 모델을 학습할 때, 일반적으로 샘플들을 “미니배치(minibatch)”로 전달하고, 매 에폭(epoch)마다 데이터를 다시 섞어서 과적합(overfit)을 막고, Python의 multiprocessing 을 사용하여 데이터 검색 속도를 높이려고 합니다. `DataLoader` 는 간단한 API로 이러한 복잡한 과정들을 추상화한 순회 가능한 객체(iterable)입니다.\n",
    "\n",
    "`DataLoader` 에 데이터셋을 불러온 뒤에는 필요에 따라 데이터셋을 순회(iterate)할 수 있습니다. 아래의 각 순회(iteration)는 (각각 batch_size=64 의 특징(feature)과 정답(label)을 포함하는) train_features 와 train_labels 의 묶음(batch)을 반환합니다. shuffle=True 로 지정했으므로, 모든 배치를 순회한 뒤 데이터가 섞입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_custom_dataset,\n",
    "                          batch_size = 64,\n",
    "                          shuffle = True)\n",
    "test_loader = DataLoader(test_custom_dataset,\n",
    "                         batch_size = 64,\n",
    "                         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since PyTorch DataLoader are generators, you can't access directly the contents unless you iterate over them in a for loop, or by explicitly creating a Python iterator using iter and consuming its elements using next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('data dictionary :', next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('x :', next(iter(train_loader))['x'])\n",
    "print('shape of x :', next(iter(train_loader))['x'].size()) # (batch size, w, h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('y :', next(iter(train_loader))['y'])\n",
    "print('shape of y :', next(iter(train_loader))['y'].size()) # (batch size,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that you'll be using during this assignment is a subset of the sign language digits. It contains six different classes representing the digits from 0 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set()\n",
    "for mini_batch in train_loader:\n",
    "    y = mini_batch['y']\n",
    "    for element in y.numpy():\n",
    "        unique_labels.add(element)\n",
    "        \n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see some of the images in the dataset by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow((train_custom_dataset[i]['x']  * 255).astype(\"uint8\").reshape(64, 64, 3))\n",
    "    plt.title(train_custom_dataset[i]['y'].astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-1'></a>\n",
    "### 2.1 - Linear Function\n",
    "\n",
    "Let's begin this programming exercise by computing the following equation: $Y = WX + b$, where $W$ and $X$ are random matrices and b is a random vector. \n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - linear_function\n",
    "\n",
    "Compute $WX + b$ where $W, X$, and $b$ are drawn from a random normal distribution. W is of shape (4, 3), X is (3,1) and b is (4,1). As an example, this is how to define a constant X with the shape (3,1):\n",
    "```python\n",
    "X = torch.Tensor(np.random.randn(3,1))\n",
    "\n",
    "```\n",
    "\n",
    "You might find the following functions helpful: \n",
    "- `torch.matmul(..., ...)` to do a matrix multiplication\n",
    "- `torch.add(..., ...)` to do an addition\n",
    "- `np.random.randn(...)` to initialize randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function():\n",
    "    \"\"\"\n",
    "    Implements a linear function: \n",
    "            Initializes X to be a random tensor of shape (3,1)\n",
    "            Initializes W to be a random tensor of shape (4,3)\n",
    "            Initializes b to be a random tensor of shape (4,1)\n",
    "    Returns: \n",
    "    result -- Y = WX + b \n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    \"\"\"\n",
    "    Note, to ensure that the \"random\" numbers generated match the expected results,\n",
    "    please create the variables in the order given in the starting code below.\n",
    "    (Do not re-arrange the order).\n",
    "    \"\"\"\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # (approx. 4 lines)\n",
    "    X = torch.Tensor(np.random.randn(3, 1))\n",
    "    W = torch.Tensor(np.random.randn(4, 3))\n",
    "    b = torch.Tensor(np.random.randn(4, 1))\n",
    "    Y = W @ X + b\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = linear_function()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "```\n",
    "tensor([[-2.1566],\n",
    "        [ 2.9589],\n",
    "        [-1.0893],\n",
    "        [-0.8454]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - Computing the Sigmoid \n",
    "Amazing! You just implemented a linear function. PyTorch offers a variety of commonly used neural network functions like `torch.nn.functional.sigmoid` and `torch.nn.functional.softmax`.\n",
    "\n",
    "For this exercise, compute the sigmoid of z. \n",
    "\n",
    "In this exercise, you will: Cast your tensor to type `float32` using `tensor.type`, then compute the sigmoid using `torch.nn.functional.sigmoid`. \n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - sigmoid\n",
    "\n",
    "Implement the sigmoid function below. You should use the following: \n",
    "\n",
    "- `z.type(torch.float32)`\n",
    "- `torch.nn.functional.sigmoid('...')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- input value, scalar or vector\n",
    "    \n",
    "    Returns: \n",
    "    a -- (torch.float32) the sigmoid of z\n",
    "    \"\"\"\n",
    "    z = torch.tensor(z)\n",
    "    # tf.keras.activations.sigmoid requires float16, float32, float64, complex64, or complex128.\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # (approx. 2 lines)\n",
    "    a = torch.nn.functional.sigmoid(z)    # YOUR CODE ENDS HERE\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sigmoid(-1)\n",
    "print (\"type: \" + str(type(result)))\n",
    "print (\"dtype: \" + str(result.dtype))\n",
    "print (\"sigmoid(-1) = \" + str(result))\n",
    "print (\"sigmoid(0) = \" + str(sigmoid(0.0)))\n",
    "print (\"sigmoid(12) = \" + str(sigmoid(12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "<table>\n",
    "<tr> \n",
    "<td>\n",
    "type\n",
    "</td>\n",
    "<td>\n",
    "class 'torch.Tensor'\n",
    "</td>\n",
    "</tr><tr> \n",
    "<td>\n",
    "dtype\n",
    "</td>\n",
    "<td>\n",
    "'torch.float32'\n",
    "</td>\n",
    "</tr>\n",
    "<tr> \n",
    "<td>\n",
    "Sigmoid(-1)\n",
    "</td>\n",
    "<td>\n",
    "0.2689\n",
    "</td>\n",
    "</tr>\n",
    "<tr> \n",
    "<td>\n",
    "Sigmoid(0)\n",
    "</td>\n",
    "<td>\n",
    "0.5000\n",
    "</td>\n",
    "</tr>\n",
    "<tr> \n",
    "<td>\n",
    "Sigmoid(12)\n",
    "</td>\n",
    "<td>\n",
    "1.0000\n",
    "</td>\n",
    "</tr> \n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-3'></a>\n",
    "### 2.3 - Using One Hot Encodings\n",
    "\n",
    "Many times in deep learning you will have a $Y$ vector with numbers ranging from $0$ to $C-1$, where $C$ is the number of classes. If $C$ is for example 4, then you might have the following y vector which you will need to convert like this:\n",
    "\n",
    "\n",
    "<img src=\"images/onehot.png\" style=\"width:600px;height:150px;\">\n",
    "\n",
    "This is called \"one hot\" encoding, because in the converted representation, exactly one element of each column is \"hot\" (meaning set to 1). To do this conversion in numpy, you might have to write a few lines of code. In PyTorch, you can use one line of code: \n",
    "\n",
    "- [torch.nn.functional.one_hot(tensor, num_classes= - 1)](https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html)\n",
    "\n",
    "`num_classes = -1` indicates the number of classes will be inferred as one greater than the largest class value in the input tensor.\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - one_hot_matrix\n",
    "\n",
    "Implement the function below to take one label and the total number of classes $C$, and return the one hot encoding in a column wise matrix. Use `torch.nn.functional.one_hot()` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(label, num_classes = 6):\n",
    "    \"\"\"\n",
    "    Computes the one hot encoding for a single label\n",
    "    \n",
    "    Arguments:\n",
    "        label --  (LongTensor) Categorical labels\n",
    "        num_classes --  (int) Number of different classes that label can take\n",
    "    \n",
    "    Returns:\n",
    "         one_hot -- torch.Tensor\n",
    "    \"\"\"\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # (approx. 1 line)\n",
    "    one_hot = nn.functional.one_hot(label, num_classes=4)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = {\n",
    "    'label' : torch.LongTensor([1]),\n",
    "    'num_classes' : 4\n",
    "}\n",
    "test_2 = {\n",
    "    'label' : torch.LongTensor([2]),\n",
    "    'num_classes' : 4\n",
    "}\n",
    "\n",
    "print('Test 1:', one_hot_matrix(test_1['label'], test_1['num_classes']))\n",
    "print('Test 2:', one_hot_matrix(test_2['label'], test_2['num_classes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "Test 1: tensor([[0, 1, 0, 0]])\n",
    "Test 2: tensor([[0, 0, 1, 0]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-4'></a>\n",
    "### 2.4 - Initialize the Parameters \n",
    "\n",
    "Now you'll initialize a vector of numbers with the Glorot(xavier normal) initializer. The function you'll be calling is `torch.nn.init.xavier_normal_`, which draws samples from a truncated normal distribution centered on 0, with `stddev = sqrt(2 / (fan_in + fan_out))`, where `fan_in` is the number of input units and `fan_out` is the number of output units, both in the weight tensor. \n",
    "\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - initialize_parameters\n",
    "\n",
    "Implement the function below to take in a shape and to return an array of numbers using the GlorotNormal(xavier normal) initializer. \n",
    "\n",
    " - `W = torch.empty(shape)`\n",
    " - `torch.nn.init.xavier_normal_(W, dtype = torch.float32)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with PyTorch. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"                  \n",
    "    \n",
    "    torch.manual_seed(1)\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    #(approx. 6 lines of code)\n",
    "    W1 = torch.nn.init.xavier_normal_(torch.empty(25, 12288, dtype = torch.float32))\n",
    "    b1 = torch.nn.init.xavier_normal_(torch.empty(25, 1, dtype = torch.float32))\n",
    "    W2 = torch.nn.init.xavier_normal_(torch.empty(12, 25, dtype = torch.float32))\n",
    "    b2 = torch.nn.init.xavier_normal_(torch.empty(12, 1, dtype = torch.float32))\n",
    "    W3 = torch.nn.init.xavier_normal_(torch.empty(6, 12, dtype = torch.float32))\n",
    "    b3 = torch.nn.init.xavier_normal_(torch.empty(6, 1, dtype = torch.float32))\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters()\n",
    "\n",
    "for key in parameters:\n",
    "    print(f'{key} shape: {tuple(parameters[key].shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "W1 shape: (25, 12288)\n",
    "b1 shape: (25, 1)\n",
    "W2 shape: (12, 25)\n",
    "b2 shape: (12, 1)\n",
    "W3 shape: (6, 12)\n",
    "b3 shape: (6, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Building Your First Neural Network in PyTorch\n",
    "\n",
    "In this part of the assignment you will build a neural network using PyTorch. Remember that there are two parts to implementing a PyTorch model:\n",
    "\n",
    "- Implement forward propagation\n",
    "- Retrieve the gradients and train the model\n",
    "\n",
    "Let's get into it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-1'></a>\n",
    "### 3.1 - Implement Forward Propagation \n",
    "\n",
    "One of PyTorch's great strengths lies in the fact that you only need to implement the forward propagation function and it will keep track of the operations you did to calculate the back propagation automatically.  \n",
    "\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - forward_propagation\n",
    "\n",
    "Implement the `forward_propagation` function.\n",
    "\n",
    "**Note** Use only the torch. \n",
    "\n",
    "- torch.add\n",
    "- torch.matmul\n",
    "- torch.nn.functional.relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    #(approx. 5 lines)                   # Numpy Equivalents:\n",
    "    # YOUR CODE STARTS HERE\n",
    "    W1 = torch.nn.init.xavier_normal_(\n",
    "        torch.empty(25, 12288, dtype=torch.float32))\n",
    "    b1 = torch.nn.init.xavier_normal_(torch.empty(25, 1, dtype=torch.float32))\n",
    "    W2 = torch.nn.init.xavier_normal_(torch.empty(12, 25, dtype=torch.float32))\n",
    "    b2 = torch.nn.init.xavier_normal_(torch.empty(12, 1, dtype=torch.float32))\n",
    "    W3 = torch.nn.init.xavier_normal_(torch.empty(6, 12, dtype=torch.float32))\n",
    "    b3 = torch.nn.init.xavier_normal_(torch.empty(6, 1, dtype=torch.float32))\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Z3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample1 = torch.FloatTensor(train_custom_dataset[0]['x'])\n",
    "test_sample2 = torch.FloatTensor(train_custom_dataset[1]['x'])\n",
    "test_samples = torch.stack([test_sample1, test_sample2], dim = -1)\n",
    "\n",
    "forward_propagation(test_samples, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "tensor([[ 0.8190,  0.8266],\n",
    "        [-0.6151, -0.3319],\n",
    "        [-1.1323, -1.1346],\n",
    "        [-1.8217, -1.6658],\n",
    "        [ 1.1146,  1.1880],\n",
    "        [ 0.4495,  0.8900]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 Compute the Cost\n",
    "\n",
    "All you have to do now is define the loss function that you're going to use. For this case, since we have a classification problem with 6 labels, a categorical cross entropy will work! \n",
    "\n",
    "<a name='ex-6'></a>\n",
    "### Exercise 6 -  compute_cost\n",
    "\n",
    "Implement the cost function below. \n",
    "- It's important to note that the \"`y_pred`\" and \"`y_true`\" inputs of [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) are expected to be of shape (number of examples, num_classes). \n",
    "\n",
    "- `tf.reduce_mean` basically does the summation over the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(logits, labels):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    logits -- output of forward propagation (output of the last LINEAR unit), of shape (6, num_examples)\n",
    "    labels -- \"true\" labels vector, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    #(1 line of code)\n",
    "    # cost = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    cost = loss(logits, labels)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.tensor([[ 2.4048107,   5.0334096 ],\n",
    "                     [-0.7921977,  -4.1523376 ],\n",
    "                     [ 0.9447198,  -0.46802214],\n",
    "                     [ 1.158121,    3.9810789 ],\n",
    "                     [ 4.768706,    2.3220146 ],\n",
    "                     [ 6.1481323,   3.909829  ]])\n",
    "true = torch.LongTensor([1, 0, 0, 1, 1, 0])\n",
    "\n",
    "compute_cost(pred, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "tensor(0.5018)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-3'></a>\n",
    "### 3.3 - Train the Model\n",
    "\n",
    "Let's talk optimizers. You'll specify the type of optimizer in one line, in this case `torch.optim.Adam` (though you can use others such as SGD), and then call it within the training loop. \n",
    "\n",
    "학습 단계(loop)에서 최적화는 세단계로 이뤄집니다.\n",
    "- `optimizer.zero_grad()`를 호출하여 모델 매개변수의 변화도를 재설정합니다. 기본적으로 변화도는 더해지기(add up) 때문에 중복 계산을 막기 위해 반복할 때마다 명시적으로 0으로 설정합니다. \n",
    "\n",
    "  \n",
    "- `loss.backwards()`를 호출하여 예측 손실(prediction loss)을 역전파합니다. PyTorch는 각 매개변수에 대한 손실의 변화도를 저장합니다.  \n",
    "\n",
    "  \n",
    "- 변화도를 계산한 뒤에는 `optimizer.step()`을 호출하여 역전파 단계에서 수집된 변화도로 매개변수를 조정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_loader, test_loader, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer PyTorch neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    train_loader -- training dataloader\n",
    "    test_loader -- test dataloader\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    print_cost -- True to print the cost every 10 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    # Initialize your parameters\n",
    "    #(1 line)\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    # set parameters' requires_grad True \n",
    "    # PyTorch's autograd will save gradient\n",
    "    W1.requires_grad = True\n",
    "    b1.requires_grad = True\n",
    "    W2.requires_grad = True\n",
    "    b2.requires_grad = True\n",
    "    W3.requires_grad = True\n",
    "    b3.requires_grad = True\n",
    "    \n",
    "    # get optimizer\n",
    "    optimizer = torch.optim.Adam([W1, b1, W2, b2, W3, b3], lr = learning_rate)\n",
    "    \n",
    "    # Do the training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_cost = 0.\n",
    "        correct = 0.\n",
    "        \n",
    "        for mini_batch in train_loader:\n",
    "            \n",
    "            x = mini_batch['x'].T.type(torch.float32)\n",
    "            y = mini_batch['y'].type(torch.long)\n",
    "            \n",
    "            # 1. predict\n",
    "            z3 = forward_propagation(x, parameters)\n",
    "            \n",
    "            # 2. loss\n",
    "            cost = compute_cost(z3.T, y)\n",
    "            \n",
    "            # 3. initialize optmizer \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 4. back propagation\n",
    "            cost.backward()\n",
    "            \n",
    "            # 5. gradient descent\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_cost += float(cost)\n",
    "            correct += (z3.T.argmax(dim = -1) == y).sum().item()\n",
    "            \n",
    "        # calculate train cost and accuracy\n",
    "        epoch_cost /= len(train_loader)\n",
    "        accuracy = correct / len(train_loader.dataset)\n",
    "        \n",
    "        # Print the cost every 10 epochs\n",
    "        if print_cost == True and epoch % 10 == 0:\n",
    "            print(f'[Epoch {epoch + 1}] \\ncost: {epoch_cost} \\naccracy: {accuracy}')\n",
    "            \n",
    "            # We evaluate the test set every 10 epochs to avoid computational overhead\n",
    "            test_correct = 0\n",
    "            for test_mini_batch in test_loader:\n",
    "                x = test_mini_batch['x'].T.type(torch.float32)\n",
    "                y = test_mini_batch['y'].type(torch.long)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    z3 = forward_propagation(x, parameters)\n",
    "                    test_correct += (z3.T.argmax(dim = -1) == y).sum().item()\n",
    "                    \n",
    "            test_accuracy = test_correct / len(test_loader.dataset)\n",
    "            print(f'Test accuracy: {test_accuracy}')\n",
    "            print()\n",
    "            \n",
    "            costs.append(epoch_cost)\n",
    "            train_acc.append(accuracy)\n",
    "            test_acc.append(test_accuracy)\n",
    "        \n",
    "    return parameters, costs, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs, train_acc, test_acc = model(train_loader, test_loader, num_epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per fives)')\n",
    "plt.title(\"Learning rate =\" + str(0.0001))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train accuracy\n",
    "plt.plot(np.squeeze(train_acc))\n",
    "plt.ylabel('Train Accuracy')\n",
    "plt.xlabel('iterations (per fives)')\n",
    "plt.title(\"Learning rate =\" + str(0.0001))\n",
    "# Plot the test accuracy\n",
    "plt.plot(np.squeeze(test_acc))\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xlabel('iterations (per fives)')\n",
    "plt.title(\"Learning rate =\" + str(0.0001))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations**! You've made it to the end of this assignment, and to the end of this week's material. Amazing work building a neural network in PyTorch!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}