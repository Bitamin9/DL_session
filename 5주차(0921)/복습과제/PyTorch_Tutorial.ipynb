{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86e4f65",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b77e96e",
   "metadata": {},
   "source": [
    "# 텐서(TENSOR)\n",
    "텐서(tensor)는 배열(array)이나 행렬(matrix)과 매우 유사한 특수한 자료구조입니다. PyTorch에서는 텐서를 사용하여 모델의 입력(input)과 출력(output), 그리고 모델의 매개변수들을 부호화(encode)합니다.\n",
    "\n",
    "텐서는 GPU나 다른 하드웨어 가속기에서 실행할 수 있다는 점만 제외하면 NumPy 의 ndarray와 유사합니다. 실제로 텐서와 NumPy 배열(array)은 종종 동일한 내부(underly) 메모리를 공유할 수 있어 데이터를 복수할 필요가 없습니다. 텐서는 또한 자동 미분(automatic differentiation)에 최적화되어 있습니다. ndarray에 익숙하다면 Tensor API를 바로 사용할 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfedb610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = h5py.File('./datasets/train_signs.h5', \"r\")\n",
    "test_dataset = h5py.File('./datasets/test_signs.h5', \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137dcec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(train_dataset['train_set_x'])\n",
    "y_train = np.array(train_dataset['train_set_y'])\n",
    "\n",
    "x_test = np.array(test_dataset['test_set_x'])\n",
    "y_test = np.array(test_dataset['test_set_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c664905",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f9756",
   "metadata": {},
   "source": [
    "## 텐서(tensor) 초기화\n",
    "\n",
    "#### 데이터로부터 직접(directly) 생성하기\n",
    "데이터로부터 직접 텐서를 생성할 수 있습니다. 데이터의 자료형(data type)은 자동으로 유추합니다.\n",
    "```python\n",
    "torch.tensor(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1913c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "# YOUR CODE STARTS HERE\n",
    "tensor_data = \n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a307c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658b18a",
   "metadata": {},
   "source": [
    "#### NumPy 배열로부터 생성하기\n",
    "텐서는 NumPy 배열로 생성할 수 있습니다.\n",
    "```python\n",
    "torch.from_numpy(np.ndarray)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array(data)\n",
    "# YOUR CODE STARTS HERE\n",
    "tensor_data = \n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e555d",
   "metadata": {},
   "source": [
    "#### 다른 텐서로부터 생성하기\n",
    "명시적으로 재정의(override)하지 않는다면, 인자로 주어진 텐서의 속성(모양(shape), 자료형(datatype))을 유지합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a0d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_ones = torch.ones_like(tensor_data)\n",
    "tensor_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_rand = torch.rand_like(tensor_data, dtype = torch.float)\n",
    "tensor_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df865048",
   "metadata": {},
   "source": [
    "## 텐서의 속성(Attribute)\n",
    "텐서의 속성은 텐서의 모양(shape), 자료형(datatype) 및 어느 장치(device)에 저장되는지를 나타냅니다.\n",
    "\n",
    "```python\n",
    "tensor.shape\n",
    "tensor.dtype\n",
    "tensor.device\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "# YOUR CODE STARTS HERE\n",
    "print(f\"Shape of tensor: {}\")\n",
    "print(f\"Datatype of tensor: {}\")\n",
    "print(f\"Device tensor is stored on: {}\")\n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6290644",
   "metadata": {},
   "source": [
    "## 텐서 연산(Operation)\n",
    "전치(transposing), 인덱싱(indexing), 슬라이싱(slicing), 수학 계산, 선형 대수, 임의 샘플링(random sampling) 등, 100가지 이상의 텐서 연산들을 여기 에서 확인할 수 있습니다.\n",
    "\n",
    "기본적으로 텐서는 CPU에 생성됩니다. `.to` 메소드를 사용하면 (GPU의 가용성(availability)을 확인한 뒤) GPU로 텐서를 명시적으로 이동할 수 있습니다. 장치들 간에 큰 텐서들을 복사하는 것은 시간과 메모리 측면에서 비용이 많이든다는 것을 기억하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f58ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU가 존재하면 텐서를 이동합니다\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b23deab",
   "metadata": {},
   "source": [
    "#### NumPy식의 표준 인덱싱과 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "tensor = torch.rand(4, 4)\n",
    "# YOUR CODE STARTS HERE\n",
    "print(f\"First row: {}\")\n",
    "print(f\"First column: {}\")\n",
    "print(f\"Last column: {}\")\n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a20ffdd",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "First row: tensor([0.7576, 0.2793, 0.4031, 0.7347])\n",
    "First column: tensor([0.7576, 0.0293, 0.5695, 0.6826])\n",
    "Last column: tensor([0.7347, 0.7544, 0.5247, 0.4550])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ccb4ba",
   "metadata": {},
   "source": [
    "#### 텐서 합치기 \n",
    "`torch.cat`을 사용하여 주어진 차원에 따라 일련의 텐서를 연결할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb42397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "tensor3 = torch.tensor([7, 8, 9])\n",
    "\n",
    "# YOUR CODE STARTS HERE\n",
    "cat_tensor = \n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc6744",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b73e64",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01df159",
   "metadata": {},
   "source": [
    "`torch.stack`을 사용하여 tensor들을 새로운 차원으로 연결할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219fd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "tensor3 = torch.tensor([7, 8, 9])\n",
    "\n",
    "# YOUR CODE STARTS HERE\n",
    "# use dim = 0\n",
    "dim0_tensor = \n",
    "# use dim = 1\n",
    "dim1_tensor = \n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527cb732",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'stack dim = 0: \\n {dim0_tensor}', end = '\\n\\n')\n",
    "print(f'stack dim = 1: \\n {dim1_tensor}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf15f3",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "stack dim = 0: \n",
    " tensor([[1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [7, 8, 9]])\n",
    "\n",
    "stack dim = 1: \n",
    " tensor([[1, 4, 7],\n",
    "        [2, 5, 8],\n",
    "        [3, 6, 9]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a04f2",
   "metadata": {},
   "source": [
    "#### 산술 연산(Arithmetic operations)\n",
    "`tensor.matmul`: 텐서 간 행렬 곱(matrix multiplication)을 계산합니다. (`@`로 사용가능)  \n",
    "\n",
    "`tensor.mul`: 요소별 곱(element-wise product)을 계산합니다.(`*`로 사용가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.rand(3, 3)\n",
    "tensor2 = torch.rand(3, 3)\n",
    "\n",
    "matmul = tensor1.matmul(tensor2)\n",
    "matmul = tensor1 @ tensor2\n",
    "\n",
    "element_wise = tensor1.mul(tensor2)\n",
    "element_wise = tensor1 + tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5697ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('matrix multiplication: \\n', matmul, end = '\\n\\n')\n",
    "print('element-wise product: \\n', element_wise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c7e735",
   "metadata": {},
   "source": [
    "# 자동미분(Autograd)\n",
    "신경망을 학습할 때 가장 자주 사용되는 알고리즘은 역전파입니다. 이 알고리즘에서, 매개변수(모델 가중치)는 주어진 매개변수에 대한 손실 함수의 변화도(gradient)에 따라 조정됩니다.\n",
    "\n",
    "이러한 변화도를 계산하기 위해 PyTorch에는 torch.autograd라고 불리는 자동 미분 엔진이 내장되어 있습니다. 이는 모든 계산 그래프에 대한 변화도의 자동 계산을 지원합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d57cfd",
   "metadata": {},
   "source": [
    "#### Linear Function\n",
    "Compute $WX + b$ where $W, X$, and $b$ are drawn from a random normal distribution. W is of shape (4, 3), X is (3,1) and b is (4,1). As an example, here is how you would define a constant X that has shape (3,1)\n",
    "\n",
    "이 신경망에서, w와 b는 최적화를 해야 하는 매개변수입니다. 따라서 이러한 변수들에 대한 손실 함수의 변화도를 계산할 수 있어야 합니다. 이를 위해서 해당 텐서에 requires_grad 속성을 설정합니다.  \n",
    "**note**: requires_grad의 값은 텐서를 생성할 때 설정하거나, 나중에 x.requires_grad_(True) 메소드를 사용하여 나중에 설정할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a9c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.FloatTensor([10, 14, 13, 15]).view(4, 1)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "X = torch.randn(3, 1)\n",
    "W = torch.randn(4, 3, requires_grad = True)\n",
    "b = torch.randn(4, 1, requires_grad = True)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "# YOUR CODE STARTS HERE\n",
    "Z = \n",
    "# YOUR CODE ENDS HERE\n",
    "cost = loss(Z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d174901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Z : \\n', Z, end = '\\n\\n')\n",
    "print('cost : \\n', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7393efb",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "Z : \n",
    " tensor([[ 0.5474],\n",
    "        [-1.3898],\n",
    "        [-1.1248],\n",
    "        [-1.9337]], grad_fn=<AddBackward0>)\n",
    "\n",
    "cost : \n",
    " tensor(203.1145, grad_fn=<MseLossBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4660a47e",
   "metadata": {},
   "source": [
    "이 코드는 다음의 연산 그래프 를 정의합니다.\n",
    "![graph](https://tutorials.pytorch.kr/_images/comp-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da0cdb4",
   "metadata": {},
   "source": [
    "#### 변화도(Gradient) 계산하기\n",
    "신경망에서 매개변수의 가중치를 최적화하려면 매개변수에 대한 손실함수의 도함수(derivative)를 계산해야 합니다. 즉, x와 y의 일부 고정값에서 $\\frac{\\partial loss}{\\partial w}$와 $\\frac{\\partial loss}{\\partial b}$가 필요합니다. 이러한 도함수를 계산하기 위해, `cost.backward()`를 호출한 다음 `W.grad`와 `b.grad`에서 값을 가져옵니다:\n",
    "\n",
    "\n",
    "- 연산 그래프의 잎(leaf) 노드들 중 `requires_grad` 속성이 True로 설정된 노드들의 grad 속성만 구할 수 있습니다. 그래프의 다른 모든 노드에서는 변화도가 유효하지 않습니다.\n",
    "\n",
    "- 성능 상의 이유로, 주어진 그래프에서의 backward를 사용한 변화도 계산은 한 번만 수행할 수 있습니다. 만약 동일한 그래프에서 여러번의 backward 호출이 필요하면, backward 호출 시에 `retrain_graph=True`를 전달해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88421c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE STARTS HERE\n",
    "\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "print(W.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff011053",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "tensor([[-3.1258, -1.2616, -0.2915],\n",
    "        [-5.0890, -2.0540, -0.4746],\n",
    "        [-4.6707, -1.8851, -0.4356],\n",
    "        [-5.5996, -2.2600, -0.5222]])\n",
    "tensor([[-4.7263],\n",
    "        [-7.6949],\n",
    "        [-7.0624],\n",
    "        [-8.4668]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477cc0d8",
   "metadata": {},
   "source": [
    "#### 변화도 추적 멈추기\n",
    "기본적으로, `requires_grad=True`인 모든 텐서들은 연산 기록을 추적하고 변화도 계산을 지원합니다. 그러나 모델을 학습한 뒤 입력 데이터를 단순히 적용하기만 하는 경우와 같이 순전파 연산만 필요한 경우에는, 이러한 추적이나 지원이 필요 없을 수 있습니다. 연산 코드를 `torch.no_grad()` 블록으로 둘러싸서 연산 추적을 멈출 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cce3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = torch.matmul(W, X) + b\n",
    "print(Z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a3407",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Z = torch.matmul(W, X) + b\n",
    "    print(Z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49819576",
   "metadata": {},
   "source": [
    "동일한 결과를 얻는 다른 방법은 텐서에 `detach()`메소드를 사용하는 것입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = torch.matmul(W, X)+b\n",
    "Z_det = Z.detach()\n",
    "print(Z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027bc96",
   "metadata": {},
   "source": [
    "# PyTorch Dataset & DataLoader\n",
    "데이터 샘플을 처리하는 코드는 지저분(messy)하고 유지보수가 어려울 수 있습니다. 더 나은 가독성(readability)과 모듈성(modularity)을 위해 데이터셋 코드를 모델 학습 코드로부터 분리하는 것이 이상적입니다. PyTorch는 `torch.utils.data.DataLoader` 와 `torch.utils.data.Dataset` 의 두 가지 데이터 기본 요소를 제공하여 미리 준비해둔(pre-loaded) 데이터셋 뿐만 아니라 가지고 있는 데이터를 사용할 수 있도록 합니다. `Dataset` 은 샘플과 정답(label)을 저장하고, `DataLoader` 는 Dataset 을 샘플에 쉽게 접근할 수 있도록 순회 가능한 객체(iterable)로 감쌉니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b826682",
   "metadata": {},
   "source": [
    "## PyTorch Dataset\n",
    "\n",
    "사용자 정의 Dataset 클래스는 반드시 3개 함수를 구현해야 합니다: \\_\\_init\\_\\_, \\_\\_len\\_\\_, and \\_\\_getitem\\_\\_. \n",
    "\n",
    "`__init__`  \n",
    "\\_\\_init\\_\\_ 함수는 Dataset 객체가 생성(instantiate)될 때 한 번만 실행됩니다. \n",
    "\n",
    "`__len__`  \n",
    "\\_\\_len\\_\\_ 함수는 데이터셋의 샘플 개수를 반환합니다.\n",
    "\n",
    "`__getitem__`  \n",
    "\\_\\_getitem\\_\\_ 함수는 주어진 인덱스 idx 에 해당하는 샘플을 데이터셋에서 불러오고 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b5414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # normalize(/255.0)\n",
    "        # YOUR CODE STARTS HERE\n",
    "        normalized_x = \n",
    "        # YOUR CODE ENDS HERE\n",
    "        return {\n",
    "            'x' : normalized_x,\n",
    "            'y' : self.y[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20570a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_custom_dataset = Custom_Dataset(x_train, y_train)\n",
    "test_custom_dataset = Custom_Dataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_custom_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcbdef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('__len__ :', len(train_custom_dataset))\n",
    "print('__len__ :', train_custom_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('__getitem__ :', train_custom_dataset[0])\n",
    "print('__getitem__ :', train_custom_dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89fa10",
   "metadata": {},
   "source": [
    "## PyTorch DataLoader\n",
    "`Dataset` 은 데이터셋의 특징(feature)을 가져오고 하나의 샘플에 정답(label)을 지정하는 일을 한 번에 합니다. 모델을 학습할 때, 일반적으로 샘플들을 “미니배치(minibatch)”로 전달하고, 매 에폭(epoch)마다 데이터를 다시 섞어서 과적합(overfit)을 막고, Python의 multiprocessing 을 사용하여 데이터 검색 속도를 높이려고 합니다. `DataLoader` 는 간단한 API로 이러한 복잡한 과정들을 추상화한 순회 가능한 객체(iterable)입니다.\n",
    "\n",
    "`DataLoader` 에 데이터셋을 불러온 뒤에는 필요에 따라 데이터셋을 순회(iterate)할 수 있습니다. 아래의 각 순회(iteration)는 (각각 batch_size=64 의 특징(feature)과 정답(label)을 포함하는) train_features 와 train_labels 의 묶음(batch)을 반환합니다. shuffle=True 로 지정했으므로, 모든 배치를 순회한 뒤 데이터가 섞입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dddcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_custom_dataset,\n",
    "                          batch_size = 64,\n",
    "                          shuffle = True)\n",
    "test_loader = DataLoader(test_custom_dataset,\n",
    "                         batch_size = 64,\n",
    "                         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cbad32",
   "metadata": {},
   "source": [
    "since PyTorch DataLoader are generators, you can't access directly the contents unless you iterate over them in a for loop, or by explicitly creating a Python iterator using iter and consuming its elements using next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a38849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data dictionary :', next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4cad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x :', next(iter(train_loader))['x'])\n",
    "print('shape of x :', next(iter(train_loader))['x'].size()) # (batch size, w, h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b212f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y :', next(iter(train_loader))['y'])\n",
    "print('shape of y :', next(iter(train_loader))['y'].size()) # (batch size,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa74d0",
   "metadata": {},
   "source": [
    "The dataset that you'll be using during this assignment is a subset of the sign language digits. It contains six different classes representing the digits from 0 to 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31133076",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set()\n",
    "for mini_batch in train_loader:\n",
    "    y = mini_batch['y']\n",
    "    for element in y.numpy():\n",
    "        unique_labels.add(element)\n",
    "        \n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2cfbde",
   "metadata": {},
   "source": [
    "You can see some of the images in the dataset by running the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0baf5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow((train_custom_dataset[i]['x']  * 255).astype(\"uint8\").reshape(64, 64, 3))\n",
    "    plt.title(train_custom_dataset[i]['y'].astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756507cd",
   "metadata": {},
   "source": [
    "# 신경망 모델 구성하기\n",
    "신경망은 데이터에 대한 연산을 수행하는 계층(layer)/모듈(module)로 구성되어 있습니다. `torch.nn` 네임스페이스는 신경망을 구성하는데 필요한 모든 구성 요소를 제공합니다. PyTorch의 모든 모듈은 `nn.Module` 의 하위 클래스(subclass) 입니다. 신경망은 다른 모듈(계층 layer)로 구성된 모듈입니다. 이러한 중첩된 구조는 복잡한 아키텍처를 쉽게 구축하고 관리할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76069aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a724d6",
   "metadata": {},
   "source": [
    "## 학습을 위한 장치 얻기\n",
    "가능한 경우 GPU와 같은 하드웨어 가속기에서 모델을 학습하려고 합니다. `torch.cuda` 를 사용할 수 있는지 확인하고 그렇지 않으면 CPU를 계속 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b10d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ffc75",
   "metadata": {},
   "source": [
    "## 클래스 정의하기\n",
    "신경망 모델을 nn.Module 의 하위클래스로 정의하고, `__init__`에서 신경망 계층들을 초기화합니다. nn.Module 을 상속받은 모든 클래스는 `forward`메소드에 입력 데이터에 대한 연산들을 구현합니다.\n",
    "- `nn.Flatten`  \n",
    "각 28x28의 2D 이미지를 784 픽셀 값을 갖는 연속된 배열로 변환합니다. \n",
    "  \n",
    "  \n",
    "- `nn.Linear`  \n",
    "저장된 가중치(weight)와 편향(bias)을 사용하여 입력에 선형 변환(linear transformation)을 적용하는 모듈(강의에서 나온 $W$행렬의 전치 형태임에 주의 하세요!)\n",
    "    - **arguments**\n",
    "        - in_features: size of input sample\n",
    "        - out_features: size of output sample\n",
    "        - bias: if set to `False`, the layer will not learn additive bias(default: `True`)\n",
    "        \n",
    "        \n",
    "- `nn.ReLU`  \n",
    "ReLU 활성화함수\n",
    "\n",
    "\n",
    "- `nn.Dropout`  \n",
    "베르누이 분포로 부터 `p`의 확률로 샘플링해 dropout을 적용\n",
    "    - **arguments**\n",
    "        - p: probability of an element to be zeroed(default: 0.5)\n",
    "        \n",
    "        \n",
    "- `nn.BatchNorm1d`  \n",
    "batch normalization을 수행하는 함수. \n",
    "$$y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$$\n",
    " \n",
    " \n",
    "- `nn.Softmax`  \n",
    "신경망의 마지막 선형 계층은 nn.Softmax 모듈에 전달될 ([-infty, infty] 범위의 원시 값(raw value)인) logits 를 반환합니다. logits는 모델의 각 분류(class)에 대한 예측 확률을 나타내도록 [0, 1] 범위로 비례하여 조정(scale)됩니다. `dim` 매개변수는 값의 합이 1이 되는 차원을 나타냅니다.   \n",
    "$$\\begin{align*}\\text{Softmax}(x_{i}) &= \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\end{align*}$$\n",
    "    - **arguments**\n",
    "        - dim: A dimension along which Softmax will be computed \n",
    "\n",
    "\n",
    "- `nn.Sequential`  \n",
    "`nn.Sequential` 은 순서를 갖는 모듈의 컨테이너입니다. 데이터는 정의된 것과 같은 순서로 모든 모듈들을 통해 전달됩니다. 순차 컨테이너(sequential container)를 사용하여 아래의 seq_modules 와 같은 신경망을 빠르게 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29331204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        '''\n",
    "        flatten\n",
    "        |W1| = (12288, 25)\n",
    "        |b1| = (25,)\n",
    "        |W2| = (25, 12)\n",
    "        |b2| = (12,)\n",
    "        |W3| = (12, 6)\n",
    "        |b3| = (6,)\n",
    "        use batch normalization and dropout(p = .1)\n",
    "        '''\n",
    "        torch.manual_seed(1)\n",
    "        self.flatten = nn.Flatten() # (batch_size, 64, 64, 3) -> (batch_size, 12288)\n",
    "        self.layers = nn.Sequential(\n",
    "            # YOUR CODE STARTS HERE\n",
    "            # linear -> batch normalization -> relu -> dropout ->\n",
    "            # linear -> batch normalization -> relu -> dropout -> linear\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # YOUR CODE ENDS HERE\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        pred = self.layers(x)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b2764c",
   "metadata": {},
   "source": [
    "`NeuralNetwork`의 인스턴스(instance)를 생성하고 이를 device로 이동한 뒤, 구조(structure)를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0891935",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "NeuralNetwork(\n",
    "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
    "  (layers): Sequential(\n",
    "    (0): Linear(in_features=12288, out_features=25, bias=True)\n",
    "    (1): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (2): ReLU()\n",
    "    (3): Dropout(p=0.1, inplace=False)\n",
    "    (4): Linear(in_features=25, out_features=12, bias=True)\n",
    "    (5): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (6): ReLU()\n",
    "    (7): Dropout(p=0.1, inplace=False)\n",
    "    (8): Linear(in_features=12, out_features=6, bias=True)\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052cd56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "x = torch.rand(3, 64, 64, 3, device = device)\n",
    "\n",
    "# forward propagation\n",
    "# YOUR CODE STARTS HERE\n",
    "\n",
    "# YOUR CODE END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e2b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9bfa8",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "tensor([[ 0.2972, -0.6080,  0.0152, -0.6040, -0.5947, -0.1909],\n",
    "        [ 0.0974, -0.3929,  0.0126, -0.5071, -0.1992,  0.1962],\n",
    "        [ 0.0900, -1.1279,  0.0965, -0.1130, -0.3079,  0.4095]],\n",
    "       grad_fn=<AddmmBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a471f",
   "metadata": {},
   "source": [
    "## 모델 매개변수\n",
    "신경망 내부의 많은 계층들은 매개변수화(parameterize) 됩니다. 즉, 학습 중에 최적화되는 가중치와 편향과 연관지어집니다. nn.Module 을 상속하면 모델 객체 내부의 모든 필드들이 자동으로 추적(track)되며, 모델의 `parameters()` 및 `named_parameters()` 메소드로 모든 매개변수에 접근할 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5797297",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.parameters())\n",
    "print()\n",
    "print(next(iter(model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f'Layer: {name}')\n",
    "    print(f'Size: {param.size()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ee1d7",
   "metadata": {},
   "source": [
    "# 모델 매개변수 최적화하기\n",
    "이제 모델과 데이터가 준비되었으니, 데이터에 매개변수를 최적화하여 모델을 학습하고, 검증하고, 테스트할 차례입니다. 모델을 학습하는 과정은 반복적인 과정을 거칩니다. (에폭(epoch)이라고 부르는) 각 반복 단계에서 모델은 출력을 추측하고, 추측과 정답 사이의 오류(손실(loss))를 계산하고, 매개변수에 대한 오류의 도함수(derivative)를 수집한 뒤, 경사하강법을 사용하여 이 파라미터들을 `최적화(optimize)`합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced48f8",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터(Hyperparameter)\n",
    "하이퍼파라미터(Hyperparameter)는 모델 최적화 과정을 제어할 수 있는 조절 가능한 매개변수입니다. 서로 다른 하이퍼파라미터 값은 모델 학습과 수렴율(convergence rate)에 영향을 미칠 수 있습니다.\n",
    "\n",
    "학습 시에는 다음과 같은 하이퍼파라미터를 정의합니다:\n",
    "- 에폭(epoch) 수 - 데이터셋을 반복하는 횟수\n",
    "\n",
    "- 배치 크기(batch size) - 매개변수가 갱신되기 전 신경망을 통해 전파된 데이터 샘플의 수\n",
    "\n",
    "- 학습률(learning rate) - 각 배치/에폭에서 모델의 매개변수를 조절하는 비율. 값이 작을수록 학습 속도가 느려지고, 값이 크면 학습 중 예측할 수 없는 동작이 발생할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af694972",
   "metadata": {},
   "source": [
    "## 손실 함수(loss function)\n",
    "학습용 데이터를 제공하면, 학습되지 않은 신경망은 정답을 제공하지 않을 확률이 높습니다. 손실 함수(loss function)는 획득한 결과와 실제 값 사이의 틀린 정도(degree of dissimilarity)를 측정하며, 학습 중에 이 값을 최소화하려고 합니다. 주어진 데이터 샘플을 입력으로 계산한 예측과 정답(label)을 비교하여 손실(loss)을 계산합니다.\n",
    "\n",
    "일반적인 손실함수에는 회귀 문제(regression task)에 사용하는 `nn.MSELoss`(평균 제곱 오차(MSE; Mean Square Error))나 분류(classification)에 사용하는 `nn.NLLLoss` (음의 로그 우도(Negative Log Likelihood)), 그리고 nn.LogSoftmax와 nn.NLLLoss를 합친 `nn.CrossEntropyLoss` 등이 있습니다.\n",
    "\n",
    "모델의 출력 로짓(logit)을 nn.CrossEntropyLoss에 전달하여 로짓(logit)을 정규화하고 예측 오류를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac54d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ccf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65b0b36",
   "metadata": {},
   "source": [
    "## 옵티마이저(Optimizer)\n",
    "최적화는 각 학습 단계에서 모델의 오류를 줄이기 위해 모델 매개변수를 조정하는 과정입니다. 최적화 알고리즘은 이 과정이 수행되는 방식을 정의합니다. 모든 최적화 절차(logic)는 optimizer 객체에 캡슐화(encapsulate)됩니다. PyTorch에는 `ADAM`이나 `RMSProp`과 같은 다른 종류의 모델과 데이터에서 더 잘 동작하는 다양한 옵티마이저가 있습니다.\n",
    "\n",
    "학습하려는 `모델의 매개변수`와 `학습률(learning rate)` 하이퍼파라미터를 등록하여 옵티마이저를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c014fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7425ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea7ffa",
   "metadata": {},
   "source": [
    "## 학습 Loop(train_loop)\n",
    "학습 단계(loop)에서 최적화는 세단계로 이뤄집니다.\n",
    "- `optimizer.zero_grad()`를 호출하여 모델 매개변수의 변화도를 재설정합니다. 기본적으로 변화도는 더해지기(add up) 때문에 중복 계산을 막기 위해 반복할 때마다 명시적으로 0으로 설정합니다.\n",
    "\n",
    "- `loss.backwards()`를 호출하여 예측 손실(prediction loss)을 역전파합니다. PyTorch는 각 매개변수에 대한 손실의 변화도를 저장합니다.\n",
    "\n",
    "- 변화도를 계산한 뒤에는 `optimizer.step()`을 호출하여 역전파 단계에서 수집된 변화도로 매개변수를 조정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_loader, model, loss_fn, optimizer):\n",
    "    epoch_loss = 0\n",
    "    for idx, mini_batch in enumerate(train_loader):\n",
    "        x = mini_batch['x'].type(torch.float32)\n",
    "        y = mini_batch['y']\n",
    "        \n",
    "        # forward propagation\n",
    "        y_hat = model(x)\n",
    "        cost = loss_fn(y_hat, y)\n",
    "        \n",
    "        # backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += float(cost)\n",
    "        \n",
    "        if idx % 5 == 0:\n",
    "            print(f'[{str(idx + 1).zfill(2)}|{len(train_loader)}]loss: {np.round(float(cost), 5)}')\n",
    "            \n",
    "    epoch_loss = epoch_loss / len(train_loader)\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b147f3c6",
   "metadata": {},
   "source": [
    "## 평가 Loop(test_loop)\n",
    "학습 Loop와 동일한 과정을 거치지만 gradient descent가 필요 없기 때문에 `torch.no_grad()`로 미분값을 계산하지 않습니다. 또한 `optimizer.zero_grad()`와 `optimizer.step()`의 과정이 필요하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92995d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(test_loader, model, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    for mini_batch in test_loader:\n",
    "        x = mini_batch['x'].type(torch.float32)\n",
    "        y = mini_batch['y']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # forward propagation\n",
    "            y_hat = model(x)\n",
    "            cost = loss_fn(y_hat, y)\n",
    "            \n",
    "            epoch_loss += float(cost)\n",
    "            correct += (y_hat.argmax(dim  = -1) == y).sum().item()\n",
    "            \n",
    "    epoch_loss = epoch_loss / len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f'Test Error: \\n Accuracy: {(100 * accuracy)}%, Loss: {epoch_loss}\\n')\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51567ad",
   "metadata": {},
   "source": [
    "## 최적화 단계(Optimization Loop)\n",
    "하이퍼파라미터를 설정한 뒤에는 최적화 단계를 통해 모델을 학습하고 최적화할 수 있습니다. 최적화 단계의 각 반복(iteration)을 에폭이라고 부릅니다.\n",
    "\n",
    "하나의 에폭은 다음 두 부분으로 구성됩니다:\n",
    "학습 단계(train loop) - 학습용 데이터셋을 반복(iterate)하고 최적의 매개변수로 수렴합니다.\n",
    "\n",
    "검증/테스트 단계(validation/test loop) - 모델 성능이 개선되고 있는지를 확인하기 위해 테스트 데이터셋을 반복(iterate)합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd2bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, test_loader, model, loss_fn, optimizer, n_epochs, plot = True):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print(f'--- Epoch {epoch} ---')\n",
    "        train_loss = train_loop(train_loader, model, loss_fn, optimizer)\n",
    "        test_loss = test_loop(test_loader, model, loss_fn)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "    if plot:\n",
    "        plt.figure(figsize = (10, 6))\n",
    "        plt.plot(range(1, n_epochs + 1), train_losses, color = 'dodgerblue', label = 'Train')\n",
    "        plt.plot(range(1, n_epochs + 1), test_losses, color = 'limegreen', label = 'Test')\n",
    "        plt.title('Loss')\n",
    "        plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_loader, test_loader, model, loss_fn, optimizer, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
