{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec9d12a7f9c4643a81325faa26dc029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31390573ceb4bcfb78c4251ae03e950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5662e49a70b34d21907cbbb1df271c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb99787984914b6882442c8d011611c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "number of training data:  60000\n",
      "number of test data  10000\n"
     ]
    }
   ],
   "source": [
    "#MNIST dataset\n",
    "\n",
    "train_dset = datasets.MNIST('../data', train=True, download=True,transform=transforms.ToTensor())\n",
    "test_dset = datasets.MNIST('../data', train=False,transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "print('number of training data: ', len(train_dset))\n",
    "print('number of test data ', len(test_dset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image\n",
      "=========================================\n",
      "Shape of image\t:  torch.Size([1, 28, 28])\n",
      "10'th row of this image\t: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3137,\n",
      "        0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000, 0.1686, 0.6039,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000])\n",
      "Label\n",
      "=========================================\n",
      "label:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#데이터 확인\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "TODO: train_dset[*] 의 숫자값을 바꾸어 데이터를 다른 걸 출력해보자.\n",
    "\n",
    "\"\"\"\n",
    "image, label = train_dset[0]\n",
    "\n",
    "print('Image')\n",
    "print('=========================================')\n",
    "print('Shape of image\\t: ', image.shape)\n",
    "print('10\\'th row of this image\\t:', image[0][9])\n",
    "\n",
    "print('Label')\n",
    "print('=========================================')\n",
    "print('label: ', label)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# squeeze() 함수 : 크기가 1인 axis 제거\n",
    "plt.imshow(image.squeeze().numpy(), cmap='gray')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: batch_size 를 바꾸어보자\n",
    "\"\"\"\n",
    "train_loader = torch.utils.data.DataLoader(train_dset, batch_size = 16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size = 16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Training\n",
    "def train(model, train_loader, optimizer, i_epoch, device):\n",
    "    model.train()\n",
    "    accurate = 0\n",
    "    for i, (image, target) in enumerate(train_loader):\n",
    "        image = image.to(device)\n",
    "        output = model(image)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        prediction = torch.argmax(output, dim=1)\n",
    "        accurate += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "\n",
    "         #러닝 커브 그리기\n",
    "        loss_value.append(loss.detach().numpy())\n",
    "\n",
    "        #100번쨰 마다 loss 출력\n",
    "        if i%100==0:\n",
    "            print(\"epoch: {}, iteration: {}, loss: {}\".format(i_epoch, i, loss.item()))\n",
    "    \n",
    "    accuracy = accurate / len(train_loader.dataset)\n",
    "    train_accuracy_value.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "def test(model, test_loader, i_epoch, device):\n",
    "    model.eval()\n",
    "    accurate = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (image, target) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            output = model(image)\n",
    "            # nll_loss : negative log likelihood loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            prediction = torch.argmax(output, dim=1)\n",
    "            # view_as : view this tensor as the same size as other\n",
    "            accurate += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "\n",
    "    accuracy = accurate / len(test_loader.dataset)\n",
    "    #러닝커브 그리기\n",
    "    test_accuracy_value.append(accuracy)\n",
    "    print(\"epoch: {},train_accuracy:{}, test_accuracy: {}\".format(i_epoch, train_accuracy_value[i_epoch], test_accuracy_value[i_epoch]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: 모델 Customization! 레이어 개수나 레이어 차원, 활성화 함수를 바꾸어보자.\n",
    "\n",
    "데이터의 형태를 주의하면서 바꾸어 볼 것~\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10),\n",
    "            nn.Sigmoid()\n",
    "           )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        return self.fc(x)\n",
    "\n",
    "# layer를 1층 늘리기 \n",
    "class net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net2, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10),\n",
    "            nn.Sigmoid()\n",
    "           )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        return self.fc(x)\n",
    "    \n",
    "# layer의 unit 수 늘리기 : 64 -> 256 \n",
    "class net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net3, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.Sigmoid()\n",
    "           )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_value = []\n",
    "train_accuracy_value = []\n",
    "test_accuracy_value = []\n",
    "model = net()\n",
    "#model2 = net2()\n",
    "#model3 = net3()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "TODO: 학습률을 바꾸어보고, Optimizer 을 바꾸어보자 (선택)\n",
    "\n",
    "\"\"\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 0, loss: 2.3064017295837402\n",
      "epoch: 0, iteration: 100, loss: 2.249382495880127\n",
      "epoch: 0, iteration: 200, loss: 2.1772119998931885\n",
      "epoch: 0, iteration: 300, loss: 2.047973155975342\n",
      "epoch: 0, iteration: 400, loss: 1.9695568084716797\n",
      "epoch: 0, iteration: 500, loss: 1.9713834524154663\n",
      "epoch: 0, iteration: 600, loss: 1.9075515270233154\n",
      "epoch: 0, iteration: 700, loss: 1.8350757360458374\n",
      "epoch: 0, iteration: 800, loss: 1.8928239345550537\n",
      "epoch: 0, iteration: 900, loss: 1.8442589044570923\n",
      "epoch: 0, iteration: 1000, loss: 1.7911731004714966\n",
      "epoch: 0, iteration: 1100, loss: 1.7168495655059814\n",
      "epoch: 0, iteration: 1200, loss: 1.754547357559204\n",
      "epoch: 0, iteration: 1300, loss: 1.6984976530075073\n",
      "epoch: 0, iteration: 1400, loss: 1.8607312440872192\n",
      "epoch: 0, iteration: 1500, loss: 1.7402653694152832\n",
      "epoch: 0, iteration: 1600, loss: 1.6485296487808228\n",
      "epoch: 0, iteration: 1700, loss: 1.6776539087295532\n",
      "epoch: 0, iteration: 1800, loss: 1.7351529598236084\n",
      "epoch: 0, iteration: 1900, loss: 1.695373296737671\n",
      "epoch: 0, iteration: 2000, loss: 1.617419719696045\n",
      "epoch: 0, iteration: 2100, loss: 1.674028992652893\n",
      "epoch: 0, iteration: 2200, loss: 1.654920220375061\n",
      "epoch: 0, iteration: 2300, loss: 1.5907217264175415\n",
      "epoch: 0, iteration: 2400, loss: 1.6766777038574219\n",
      "epoch: 0, iteration: 2500, loss: 1.5858482122421265\n",
      "epoch: 0, iteration: 2600, loss: 1.6778666973114014\n",
      "epoch: 0, iteration: 2700, loss: 1.6059184074401855\n",
      "epoch: 0, iteration: 2800, loss: 1.6683876514434814\n",
      "epoch: 0, iteration: 2900, loss: 1.5752744674682617\n",
      "epoch: 0, iteration: 3000, loss: 1.6772762537002563\n",
      "epoch: 0, iteration: 3100, loss: 1.5916410684585571\n",
      "epoch: 0, iteration: 3200, loss: 1.6305245161056519\n",
      "epoch: 0, iteration: 3300, loss: 1.5984549522399902\n",
      "epoch: 0, iteration: 3400, loss: 1.6897863149642944\n",
      "epoch: 0, iteration: 3500, loss: 1.6451884508132935\n",
      "epoch: 0, iteration: 3600, loss: 1.5764654874801636\n",
      "epoch: 0, iteration: 3700, loss: 1.6246724128723145\n",
      "epoch: 0,train_accuracy:0.8234166666666667, test_accuracy: 0.8941\n",
      "epoch: 1, iteration: 0, loss: 1.6328094005584717\n",
      "epoch: 1, iteration: 100, loss: 1.5531333684921265\n",
      "epoch: 1, iteration: 200, loss: 1.6126177310943604\n",
      "epoch: 1, iteration: 300, loss: 1.6319904327392578\n",
      "epoch: 1, iteration: 400, loss: 1.6405329704284668\n",
      "epoch: 1, iteration: 500, loss: 1.5450366735458374\n",
      "epoch: 1, iteration: 600, loss: 1.587544322013855\n",
      "epoch: 1, iteration: 700, loss: 1.7792882919311523\n",
      "epoch: 1, iteration: 800, loss: 1.6006582975387573\n",
      "epoch: 1, iteration: 900, loss: 1.592645525932312\n",
      "epoch: 1, iteration: 1000, loss: 1.6586310863494873\n",
      "epoch: 1, iteration: 1100, loss: 1.5420684814453125\n",
      "epoch: 1, iteration: 1200, loss: 1.5585050582885742\n",
      "epoch: 1, iteration: 1300, loss: 1.6520965099334717\n",
      "epoch: 1, iteration: 1400, loss: 1.6007966995239258\n",
      "epoch: 1, iteration: 1500, loss: 1.595369815826416\n",
      "epoch: 1, iteration: 1600, loss: 1.6536325216293335\n",
      "epoch: 1, iteration: 1700, loss: 1.63462495803833\n",
      "epoch: 1, iteration: 1800, loss: 1.5565176010131836\n",
      "epoch: 1, iteration: 1900, loss: 1.6289292573928833\n",
      "epoch: 1, iteration: 2000, loss: 1.5792646408081055\n",
      "epoch: 1, iteration: 2100, loss: 1.6835432052612305\n",
      "epoch: 1, iteration: 2200, loss: 1.64241361618042\n",
      "epoch: 1, iteration: 2300, loss: 1.646131992340088\n",
      "epoch: 1, iteration: 2400, loss: 1.5376756191253662\n",
      "epoch: 1, iteration: 2500, loss: 1.5289241075515747\n",
      "epoch: 1, iteration: 2600, loss: 1.5215646028518677\n",
      "epoch: 1, iteration: 2700, loss: 1.602425456047058\n",
      "epoch: 1, iteration: 2800, loss: 1.6651115417480469\n",
      "epoch: 1, iteration: 2900, loss: 1.5458317995071411\n",
      "epoch: 1, iteration: 3000, loss: 1.5242584943771362\n",
      "epoch: 1, iteration: 3100, loss: 1.5488202571868896\n",
      "epoch: 1, iteration: 3200, loss: 1.668148159980774\n",
      "epoch: 1, iteration: 3300, loss: 1.5695518255233765\n",
      "epoch: 1, iteration: 3400, loss: 1.5038769245147705\n",
      "epoch: 1, iteration: 3500, loss: 1.6623387336730957\n",
      "epoch: 1, iteration: 3600, loss: 1.5912363529205322\n",
      "epoch: 1, iteration: 3700, loss: 1.490972638130188\n",
      "epoch: 1,train_accuracy:0.89445, test_accuracy: 0.9106\n",
      "epoch: 2, iteration: 0, loss: 1.5359580516815186\n",
      "epoch: 2, iteration: 100, loss: 1.4976505041122437\n",
      "epoch: 2, iteration: 200, loss: 1.5490458011627197\n",
      "epoch: 2, iteration: 300, loss: 1.5686097145080566\n",
      "epoch: 2, iteration: 400, loss: 1.5405489206314087\n",
      "epoch: 2, iteration: 500, loss: 1.5309576988220215\n",
      "epoch: 2, iteration: 600, loss: 1.5787537097930908\n",
      "epoch: 2, iteration: 700, loss: 1.580103874206543\n",
      "epoch: 2, iteration: 800, loss: 1.654290795326233\n",
      "epoch: 2, iteration: 900, loss: 1.6337833404541016\n",
      "epoch: 2, iteration: 1000, loss: 1.556912899017334\n",
      "epoch: 2, iteration: 1100, loss: 1.5271433591842651\n",
      "epoch: 2, iteration: 1200, loss: 1.5182050466537476\n",
      "epoch: 2, iteration: 1300, loss: 1.5951234102249146\n",
      "epoch: 2, iteration: 1400, loss: 1.5470948219299316\n",
      "epoch: 2, iteration: 1500, loss: 1.6745414733886719\n",
      "epoch: 2, iteration: 1600, loss: 1.636562466621399\n",
      "epoch: 2, iteration: 1700, loss: 1.6328412294387817\n",
      "epoch: 2, iteration: 1800, loss: 1.5753087997436523\n",
      "epoch: 2, iteration: 1900, loss: 1.6029053926467896\n",
      "epoch: 2, iteration: 2000, loss: 1.5059236288070679\n",
      "epoch: 2, iteration: 2100, loss: 1.533555507659912\n",
      "epoch: 2, iteration: 2200, loss: 1.522183895111084\n",
      "epoch: 2, iteration: 2300, loss: 1.5555402040481567\n",
      "epoch: 2, iteration: 2400, loss: 1.612525463104248\n",
      "epoch: 2, iteration: 2500, loss: 1.5720497369766235\n",
      "epoch: 2, iteration: 2600, loss: 1.532549500465393\n",
      "epoch: 2, iteration: 2700, loss: 1.539507269859314\n",
      "epoch: 2, iteration: 2800, loss: 1.5568816661834717\n",
      "epoch: 2, iteration: 2900, loss: 1.5091506242752075\n",
      "epoch: 2, iteration: 3000, loss: 1.5309697389602661\n",
      "epoch: 2, iteration: 3100, loss: 1.6871299743652344\n",
      "epoch: 2, iteration: 3200, loss: 1.506352186203003\n",
      "epoch: 2, iteration: 3300, loss: 1.5597095489501953\n",
      "epoch: 2, iteration: 3400, loss: 1.5658702850341797\n",
      "epoch: 2, iteration: 3500, loss: 1.5337753295898438\n",
      "epoch: 2, iteration: 3600, loss: 1.5485889911651611\n",
      "epoch: 2, iteration: 3700, loss: 1.5001444816589355\n",
      "epoch: 2,train_accuracy:0.9089833333333334, test_accuracy: 0.9182\n",
      "epoch: 3, iteration: 0, loss: 1.5328518152236938\n",
      "epoch: 3, iteration: 100, loss: 1.6256719827651978\n",
      "epoch: 3, iteration: 200, loss: 1.5013272762298584\n",
      "epoch: 3, iteration: 300, loss: 1.5703593492507935\n",
      "epoch: 3, iteration: 400, loss: 1.536840796470642\n",
      "epoch: 3, iteration: 500, loss: 1.5140087604522705\n",
      "epoch: 3, iteration: 600, loss: 1.5367152690887451\n",
      "epoch: 3, iteration: 700, loss: 1.6124703884124756\n",
      "epoch: 3, iteration: 800, loss: 1.4817991256713867\n",
      "epoch: 3, iteration: 900, loss: 1.5356110334396362\n",
      "epoch: 3, iteration: 1000, loss: 1.5375137329101562\n",
      "epoch: 3, iteration: 1100, loss: 1.5395958423614502\n",
      "epoch: 3, iteration: 1200, loss: 1.5077077150344849\n",
      "epoch: 3, iteration: 1300, loss: 1.5447078943252563\n",
      "epoch: 3, iteration: 1400, loss: 1.6097465753555298\n",
      "epoch: 3, iteration: 1500, loss: 1.4975560903549194\n",
      "epoch: 3, iteration: 1600, loss: 1.5017253160476685\n",
      "epoch: 3, iteration: 1700, loss: 1.6052050590515137\n",
      "epoch: 3, iteration: 1800, loss: 1.5920724868774414\n",
      "epoch: 3, iteration: 1900, loss: 1.5176968574523926\n",
      "epoch: 3, iteration: 2000, loss: 1.5971051454544067\n",
      "epoch: 3, iteration: 2100, loss: 1.6226273775100708\n",
      "epoch: 3, iteration: 2200, loss: 1.5479449033737183\n",
      "epoch: 3, iteration: 2300, loss: 1.5080358982086182\n",
      "epoch: 3, iteration: 2400, loss: 1.5460096597671509\n",
      "epoch: 3, iteration: 2500, loss: 1.4978699684143066\n",
      "epoch: 3, iteration: 2600, loss: 1.6724687814712524\n",
      "epoch: 3, iteration: 2700, loss: 1.5869851112365723\n",
      "epoch: 3, iteration: 2800, loss: 1.5371496677398682\n",
      "epoch: 3, iteration: 2900, loss: 1.4930922985076904\n",
      "epoch: 3, iteration: 3000, loss: 1.4920506477355957\n",
      "epoch: 3, iteration: 3100, loss: 1.5466828346252441\n",
      "epoch: 3, iteration: 3200, loss: 1.548279047012329\n",
      "epoch: 3, iteration: 3300, loss: 1.4906445741653442\n",
      "epoch: 3, iteration: 3400, loss: 1.583842158317566\n",
      "epoch: 3, iteration: 3500, loss: 1.5140389204025269\n",
      "epoch: 3, iteration: 3600, loss: 1.5514940023422241\n",
      "epoch: 3, iteration: 3700, loss: 1.5253701210021973\n",
      "epoch: 3,train_accuracy:0.9175666666666666, test_accuracy: 0.924\n",
      "epoch: 4, iteration: 0, loss: 1.5485930442810059\n",
      "epoch: 4, iteration: 100, loss: 1.5922893285751343\n",
      "epoch: 4, iteration: 200, loss: 1.615336298942566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, iteration: 300, loss: 1.656833529472351\n",
      "epoch: 4, iteration: 400, loss: 1.5545992851257324\n",
      "epoch: 4, iteration: 500, loss: 1.5228227376937866\n",
      "epoch: 4, iteration: 600, loss: 1.5566644668579102\n",
      "epoch: 4, iteration: 700, loss: 1.473783016204834\n",
      "epoch: 4, iteration: 800, loss: 1.50018310546875\n",
      "epoch: 4, iteration: 900, loss: 1.495585322380066\n",
      "epoch: 4, iteration: 1000, loss: 1.5313924551010132\n",
      "epoch: 4, iteration: 1100, loss: 1.5330760478973389\n",
      "epoch: 4, iteration: 1200, loss: 1.5101256370544434\n",
      "epoch: 4, iteration: 1300, loss: 1.567867398262024\n",
      "epoch: 4, iteration: 1400, loss: 1.6696711778640747\n",
      "epoch: 4, iteration: 1500, loss: 1.5117398500442505\n",
      "epoch: 4, iteration: 1600, loss: 1.5267553329467773\n",
      "epoch: 4, iteration: 1700, loss: 1.57245671749115\n",
      "epoch: 4, iteration: 1800, loss: 1.4903286695480347\n",
      "epoch: 4, iteration: 1900, loss: 1.535341501235962\n",
      "epoch: 4, iteration: 2000, loss: 1.5630894899368286\n",
      "epoch: 4, iteration: 2100, loss: 1.5885655879974365\n",
      "epoch: 4, iteration: 2200, loss: 1.597506046295166\n",
      "epoch: 4, iteration: 2300, loss: 1.5570015907287598\n",
      "epoch: 4, iteration: 2400, loss: 1.530285358428955\n",
      "epoch: 4, iteration: 2500, loss: 1.500589370727539\n",
      "epoch: 4, iteration: 2600, loss: 1.6289492845535278\n",
      "epoch: 4, iteration: 2700, loss: 1.519158959388733\n",
      "epoch: 4, iteration: 2800, loss: 1.610407829284668\n",
      "epoch: 4, iteration: 2900, loss: 1.4963276386260986\n",
      "epoch: 4, iteration: 3000, loss: 1.5678272247314453\n",
      "epoch: 4, iteration: 3100, loss: 1.512181282043457\n",
      "epoch: 4, iteration: 3200, loss: 1.5238978862762451\n",
      "epoch: 4, iteration: 3300, loss: 1.5814392566680908\n",
      "epoch: 4, iteration: 3400, loss: 1.515087604522705\n",
      "epoch: 4, iteration: 3500, loss: 1.5169380903244019\n",
      "epoch: 4, iteration: 3600, loss: 1.621832013130188\n",
      "epoch: 4, iteration: 3700, loss: 1.5183192491531372\n",
      "epoch: 4,train_accuracy:0.9240666666666667, test_accuracy: 0.9269\n",
      "epoch: 5, iteration: 0, loss: 1.494706153869629\n",
      "epoch: 5, iteration: 100, loss: 1.554358959197998\n",
      "epoch: 5, iteration: 200, loss: 1.496917486190796\n",
      "epoch: 5, iteration: 300, loss: 1.5193557739257812\n",
      "epoch: 5, iteration: 400, loss: 1.535570502281189\n",
      "epoch: 5, iteration: 500, loss: 1.5521440505981445\n",
      "epoch: 5, iteration: 600, loss: 1.6108226776123047\n",
      "epoch: 5, iteration: 700, loss: 1.5231879949569702\n",
      "epoch: 5, iteration: 800, loss: 1.5633565187454224\n",
      "epoch: 5, iteration: 900, loss: 1.5686571598052979\n",
      "epoch: 5, iteration: 1000, loss: 1.5474132299423218\n",
      "epoch: 5, iteration: 1100, loss: 1.4915711879730225\n",
      "epoch: 5, iteration: 1200, loss: 1.5889265537261963\n",
      "epoch: 5, iteration: 1300, loss: 1.5538337230682373\n",
      "epoch: 5, iteration: 1400, loss: 1.5023292303085327\n",
      "epoch: 5, iteration: 1500, loss: 1.5318596363067627\n",
      "epoch: 5, iteration: 1600, loss: 1.5784703493118286\n",
      "epoch: 5, iteration: 1700, loss: 1.5108819007873535\n",
      "epoch: 5, iteration: 1800, loss: 1.5279662609100342\n",
      "epoch: 5, iteration: 1900, loss: 1.5276317596435547\n",
      "epoch: 5, iteration: 2000, loss: 1.488951325416565\n",
      "epoch: 5, iteration: 2100, loss: 1.5764328241348267\n",
      "epoch: 5, iteration: 2200, loss: 1.5519177913665771\n",
      "epoch: 5, iteration: 2300, loss: 1.490302324295044\n",
      "epoch: 5, iteration: 2400, loss: 1.502008318901062\n",
      "epoch: 5, iteration: 2500, loss: 1.5232081413269043\n",
      "epoch: 5, iteration: 2600, loss: 1.5035847425460815\n",
      "epoch: 5, iteration: 2700, loss: 1.660308599472046\n",
      "epoch: 5, iteration: 2800, loss: 1.5261194705963135\n",
      "epoch: 5, iteration: 2900, loss: 1.4950439929962158\n",
      "epoch: 5, iteration: 3000, loss: 1.4965074062347412\n",
      "epoch: 5, iteration: 3100, loss: 1.5026183128356934\n",
      "epoch: 5, iteration: 3200, loss: 1.5274823904037476\n",
      "epoch: 5, iteration: 3300, loss: 1.4986488819122314\n",
      "epoch: 5, iteration: 3400, loss: 1.6187307834625244\n",
      "epoch: 5, iteration: 3500, loss: 1.4938632249832153\n",
      "epoch: 5, iteration: 3600, loss: 1.4972361326217651\n",
      "epoch: 5, iteration: 3700, loss: 1.5597858428955078\n",
      "epoch: 5,train_accuracy:0.9291833333333334, test_accuracy: 0.9316\n",
      "epoch: 6, iteration: 0, loss: 1.5325642824172974\n",
      "epoch: 6, iteration: 100, loss: 1.5528783798217773\n",
      "epoch: 6, iteration: 200, loss: 1.4848705530166626\n",
      "epoch: 6, iteration: 300, loss: 1.5266692638397217\n",
      "epoch: 6, iteration: 400, loss: 1.4866236448287964\n",
      "epoch: 6, iteration: 500, loss: 1.5362050533294678\n",
      "epoch: 6, iteration: 600, loss: 1.576712727546692\n",
      "epoch: 6, iteration: 700, loss: 1.5659842491149902\n",
      "epoch: 6, iteration: 800, loss: 1.5912818908691406\n",
      "epoch: 6, iteration: 900, loss: 1.4824968576431274\n",
      "epoch: 6, iteration: 1000, loss: 1.4699232578277588\n",
      "epoch: 6, iteration: 1100, loss: 1.5339194536209106\n",
      "epoch: 6, iteration: 1200, loss: 1.7034876346588135\n",
      "epoch: 6, iteration: 1300, loss: 1.4873573780059814\n",
      "epoch: 6, iteration: 1400, loss: 1.5181472301483154\n",
      "epoch: 6, iteration: 1500, loss: 1.5221927165985107\n",
      "epoch: 6, iteration: 1600, loss: 1.561822772026062\n",
      "epoch: 6, iteration: 1700, loss: 1.5761864185333252\n",
      "epoch: 6, iteration: 1800, loss: 1.5102674961090088\n",
      "epoch: 6, iteration: 1900, loss: 1.5697470903396606\n",
      "epoch: 6, iteration: 2000, loss: 1.548750400543213\n",
      "epoch: 6, iteration: 2100, loss: 1.4826195240020752\n",
      "epoch: 6, iteration: 2200, loss: 1.6145918369293213\n",
      "epoch: 6, iteration: 2300, loss: 1.5429494380950928\n",
      "epoch: 6, iteration: 2400, loss: 1.486702561378479\n",
      "epoch: 6, iteration: 2500, loss: 1.5372910499572754\n",
      "epoch: 6, iteration: 2600, loss: 1.5228687524795532\n",
      "epoch: 6, iteration: 2700, loss: 1.4854551553726196\n",
      "epoch: 6, iteration: 2800, loss: 1.561980962753296\n",
      "epoch: 6, iteration: 2900, loss: 1.5457911491394043\n",
      "epoch: 6, iteration: 3000, loss: 1.4920356273651123\n",
      "epoch: 6, iteration: 3100, loss: 1.5934118032455444\n",
      "epoch: 6, iteration: 3200, loss: 1.497337818145752\n",
      "epoch: 6, iteration: 3300, loss: 1.5357651710510254\n",
      "epoch: 6, iteration: 3400, loss: 1.4878753423690796\n",
      "epoch: 6, iteration: 3500, loss: 1.5552306175231934\n",
      "epoch: 6, iteration: 3600, loss: 1.543677568435669\n",
      "epoch: 6, iteration: 3700, loss: 1.4793328046798706\n",
      "epoch: 6,train_accuracy:0.9332833333333334, test_accuracy: 0.9342\n",
      "epoch: 7, iteration: 0, loss: 1.4914401769638062\n",
      "epoch: 7, iteration: 100, loss: 1.4844484329223633\n",
      "epoch: 7, iteration: 200, loss: 1.4906703233718872\n",
      "epoch: 7, iteration: 300, loss: 1.5387212038040161\n",
      "epoch: 7, iteration: 400, loss: 1.4952954053878784\n",
      "epoch: 7, iteration: 500, loss: 1.6762892007827759\n",
      "epoch: 7, iteration: 600, loss: 1.4874225854873657\n",
      "epoch: 7, iteration: 700, loss: 1.5009528398513794\n",
      "epoch: 7, iteration: 800, loss: 1.4999748468399048\n",
      "epoch: 7, iteration: 900, loss: 1.510612964630127\n",
      "epoch: 7, iteration: 1000, loss: 1.5193454027175903\n",
      "epoch: 7, iteration: 1100, loss: 1.5124380588531494\n",
      "epoch: 7, iteration: 1200, loss: 1.5147497653961182\n",
      "epoch: 7, iteration: 1300, loss: 1.4717191457748413\n",
      "epoch: 7, iteration: 1400, loss: 1.4825645685195923\n",
      "epoch: 7, iteration: 1500, loss: 1.47987961769104\n",
      "epoch: 7, iteration: 1600, loss: 1.4936237335205078\n",
      "epoch: 7, iteration: 1700, loss: 1.528156042098999\n",
      "epoch: 7, iteration: 1800, loss: 1.514528751373291\n",
      "epoch: 7, iteration: 1900, loss: 1.5718111991882324\n",
      "epoch: 7, iteration: 2000, loss: 1.4807262420654297\n",
      "epoch: 7, iteration: 2100, loss: 1.492519736289978\n",
      "epoch: 7, iteration: 2200, loss: 1.593032956123352\n",
      "epoch: 7, iteration: 2300, loss: 1.4963425397872925\n",
      "epoch: 7, iteration: 2400, loss: 1.6162923574447632\n",
      "epoch: 7, iteration: 2500, loss: 1.4976333379745483\n",
      "epoch: 7, iteration: 2600, loss: 1.5497376918792725\n",
      "epoch: 7, iteration: 2700, loss: 1.605485439300537\n",
      "epoch: 7, iteration: 2800, loss: 1.5548205375671387\n",
      "epoch: 7, iteration: 2900, loss: 1.5828313827514648\n",
      "epoch: 7, iteration: 3000, loss: 1.5420458316802979\n",
      "epoch: 7, iteration: 3100, loss: 1.4717575311660767\n",
      "epoch: 7, iteration: 3200, loss: 1.4929014444351196\n",
      "epoch: 7, iteration: 3300, loss: 1.5435792207717896\n",
      "epoch: 7, iteration: 3400, loss: 1.6023008823394775\n",
      "epoch: 7, iteration: 3500, loss: 1.5506970882415771\n",
      "epoch: 7, iteration: 3600, loss: 1.5706052780151367\n",
      "epoch: 7, iteration: 3700, loss: 1.489491581916809\n",
      "epoch: 7,train_accuracy:0.9367666666666666, test_accuracy: 0.9358\n",
      "epoch: 8, iteration: 0, loss: 1.5542144775390625\n",
      "epoch: 8, iteration: 100, loss: 1.5074353218078613\n",
      "epoch: 8, iteration: 200, loss: 1.5325220823287964\n",
      "epoch: 8, iteration: 300, loss: 1.5011869668960571\n",
      "epoch: 8, iteration: 400, loss: 1.5099270343780518\n",
      "epoch: 8, iteration: 500, loss: 1.4923529624938965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, iteration: 600, loss: 1.6072605848312378\n",
      "epoch: 8, iteration: 700, loss: 1.5143940448760986\n",
      "epoch: 8, iteration: 800, loss: 1.553816795349121\n",
      "epoch: 8, iteration: 900, loss: 1.5211927890777588\n",
      "epoch: 8, iteration: 1000, loss: 1.4827430248260498\n",
      "epoch: 8, iteration: 1100, loss: 1.5191879272460938\n",
      "epoch: 8, iteration: 1200, loss: 1.5580663681030273\n",
      "epoch: 8, iteration: 1300, loss: 1.4852668046951294\n",
      "epoch: 8, iteration: 1400, loss: 1.570419430732727\n",
      "epoch: 8, iteration: 1500, loss: 1.579175353050232\n",
      "epoch: 8, iteration: 1600, loss: 1.4835487604141235\n",
      "epoch: 8, iteration: 1700, loss: 1.4734584093093872\n",
      "epoch: 8, iteration: 1800, loss: 1.4945529699325562\n",
      "epoch: 8, iteration: 1900, loss: 1.4766404628753662\n",
      "epoch: 8, iteration: 2000, loss: 1.6787621974945068\n",
      "epoch: 8, iteration: 2100, loss: 1.4806169271469116\n",
      "epoch: 8, iteration: 2200, loss: 1.5895334482192993\n",
      "epoch: 8, iteration: 2300, loss: 1.4727270603179932\n",
      "epoch: 8, iteration: 2400, loss: 1.5615180730819702\n",
      "epoch: 8, iteration: 2500, loss: 1.4870413541793823\n",
      "epoch: 8, iteration: 2600, loss: 1.5627223253250122\n",
      "epoch: 8, iteration: 2700, loss: 1.5328449010849\n",
      "epoch: 8, iteration: 2800, loss: 1.4721002578735352\n",
      "epoch: 8, iteration: 2900, loss: 1.4726731777191162\n",
      "epoch: 8, iteration: 3000, loss: 1.4714871644973755\n",
      "epoch: 8, iteration: 3100, loss: 1.4747942686080933\n",
      "epoch: 8, iteration: 3200, loss: 1.5241913795471191\n",
      "epoch: 8, iteration: 3300, loss: 1.5216548442840576\n",
      "epoch: 8, iteration: 3400, loss: 1.4761860370635986\n",
      "epoch: 8, iteration: 3500, loss: 1.4876290559768677\n",
      "epoch: 8, iteration: 3600, loss: 1.4825644493103027\n",
      "epoch: 8, iteration: 3700, loss: 1.4986591339111328\n",
      "epoch: 8,train_accuracy:0.9397, test_accuracy: 0.9384\n",
      "epoch: 9, iteration: 0, loss: 1.471173644065857\n",
      "epoch: 9, iteration: 100, loss: 1.4857364892959595\n",
      "epoch: 9, iteration: 200, loss: 1.6239638328552246\n",
      "epoch: 9, iteration: 300, loss: 1.5232717990875244\n",
      "epoch: 9, iteration: 400, loss: 1.496070146560669\n",
      "epoch: 9, iteration: 500, loss: 1.5014301538467407\n",
      "epoch: 9, iteration: 600, loss: 1.530639410018921\n",
      "epoch: 9, iteration: 700, loss: 1.5741835832595825\n",
      "epoch: 9, iteration: 800, loss: 1.509900450706482\n",
      "epoch: 9, iteration: 900, loss: 1.5956207513809204\n",
      "epoch: 9, iteration: 1000, loss: 1.5780961513519287\n",
      "epoch: 9, iteration: 1100, loss: 1.533072829246521\n",
      "epoch: 9, iteration: 1200, loss: 1.541837453842163\n",
      "epoch: 9, iteration: 1300, loss: 1.4936012029647827\n",
      "epoch: 9, iteration: 1400, loss: 1.4920659065246582\n",
      "epoch: 9, iteration: 1500, loss: 1.6049655675888062\n",
      "epoch: 9, iteration: 1600, loss: 1.466539978981018\n",
      "epoch: 9, iteration: 1700, loss: 1.5452708005905151\n",
      "epoch: 9, iteration: 1800, loss: 1.547450065612793\n",
      "epoch: 9, iteration: 1900, loss: 1.487445592880249\n",
      "epoch: 9, iteration: 2000, loss: 1.5570553541183472\n",
      "epoch: 9, iteration: 2100, loss: 1.5495843887329102\n",
      "epoch: 9, iteration: 2200, loss: 1.5902562141418457\n",
      "epoch: 9, iteration: 2300, loss: 1.6055408716201782\n",
      "epoch: 9, iteration: 2400, loss: 1.4810041189193726\n",
      "epoch: 9, iteration: 2500, loss: 1.4685063362121582\n",
      "epoch: 9, iteration: 2600, loss: 1.5505592823028564\n",
      "epoch: 9, iteration: 2700, loss: 1.5368589162826538\n",
      "epoch: 9, iteration: 2800, loss: 1.5093939304351807\n",
      "epoch: 9, iteration: 2900, loss: 1.5153635740280151\n",
      "epoch: 9, iteration: 3000, loss: 1.5098217725753784\n",
      "epoch: 9, iteration: 3100, loss: 1.4671261310577393\n",
      "epoch: 9, iteration: 3200, loss: 1.5020684003829956\n",
      "epoch: 9, iteration: 3300, loss: 1.5538958311080933\n",
      "epoch: 9, iteration: 3400, loss: 1.4854552745819092\n",
      "epoch: 9, iteration: 3500, loss: 1.482530951499939\n",
      "epoch: 9, iteration: 3600, loss: 1.5187894105911255\n",
      "epoch: 9, iteration: 3700, loss: 1.5115723609924316\n",
      "epoch: 9,train_accuracy:0.9425833333333333, test_accuracy: 0.9409\n",
      "epoch: 10, iteration: 0, loss: 1.5230276584625244\n",
      "epoch: 10, iteration: 100, loss: 1.4803507328033447\n",
      "epoch: 10, iteration: 200, loss: 1.5352716445922852\n",
      "epoch: 10, iteration: 300, loss: 1.5048037767410278\n",
      "epoch: 10, iteration: 400, loss: 1.5403584241867065\n",
      "epoch: 10, iteration: 500, loss: 1.493850588798523\n",
      "epoch: 10, iteration: 600, loss: 1.4980860948562622\n",
      "epoch: 10, iteration: 700, loss: 1.4869863986968994\n",
      "epoch: 10, iteration: 800, loss: 1.485855221748352\n",
      "epoch: 10, iteration: 900, loss: 1.5642434358596802\n",
      "epoch: 10, iteration: 1000, loss: 1.5310877561569214\n",
      "epoch: 10, iteration: 1100, loss: 1.4882762432098389\n",
      "epoch: 10, iteration: 1200, loss: 1.4766041040420532\n",
      "epoch: 10, iteration: 1300, loss: 1.4733747243881226\n",
      "epoch: 10, iteration: 1400, loss: 1.5148924589157104\n",
      "epoch: 10, iteration: 1500, loss: 1.5323032140731812\n",
      "epoch: 10, iteration: 1600, loss: 1.4700584411621094\n",
      "epoch: 10, iteration: 1700, loss: 1.6348795890808105\n",
      "epoch: 10, iteration: 1800, loss: 1.4756113290786743\n",
      "epoch: 10, iteration: 1900, loss: 1.5074353218078613\n",
      "epoch: 10, iteration: 2000, loss: 1.5602799654006958\n",
      "epoch: 10, iteration: 2100, loss: 1.4789763689041138\n",
      "epoch: 10, iteration: 2200, loss: 1.5804072618484497\n",
      "epoch: 10, iteration: 2300, loss: 1.508002519607544\n",
      "epoch: 10, iteration: 2400, loss: 1.4731138944625854\n",
      "epoch: 10, iteration: 2500, loss: 1.553534746170044\n",
      "epoch: 10, iteration: 2600, loss: 1.5099527835845947\n",
      "epoch: 10, iteration: 2700, loss: 1.5377167463302612\n",
      "epoch: 10, iteration: 2800, loss: 1.5423029661178589\n",
      "epoch: 10, iteration: 2900, loss: 1.4686545133590698\n",
      "epoch: 10, iteration: 3000, loss: 1.4901987314224243\n",
      "epoch: 10, iteration: 3100, loss: 1.479519248008728\n",
      "epoch: 10, iteration: 3200, loss: 1.5341793298721313\n",
      "epoch: 10, iteration: 3300, loss: 1.5026301145553589\n",
      "epoch: 10, iteration: 3400, loss: 1.4802590608596802\n",
      "epoch: 10, iteration: 3500, loss: 1.4734348058700562\n",
      "epoch: 10, iteration: 3600, loss: 1.4888596534729004\n",
      "epoch: 10, iteration: 3700, loss: 1.5347219705581665\n",
      "epoch: 10,train_accuracy:0.9450166666666666, test_accuracy: 0.9431\n",
      "epoch: 11, iteration: 0, loss: 1.481419563293457\n",
      "epoch: 11, iteration: 100, loss: 1.5301992893218994\n",
      "epoch: 11, iteration: 200, loss: 1.478674054145813\n",
      "epoch: 11, iteration: 300, loss: 1.5807323455810547\n",
      "epoch: 11, iteration: 400, loss: 1.5451152324676514\n",
      "epoch: 11, iteration: 500, loss: 1.4901517629623413\n",
      "epoch: 11, iteration: 600, loss: 1.5536929368972778\n",
      "epoch: 11, iteration: 700, loss: 1.5137126445770264\n",
      "epoch: 11, iteration: 800, loss: 1.5428584814071655\n",
      "epoch: 11, iteration: 900, loss: 1.5099190473556519\n",
      "epoch: 11, iteration: 1000, loss: 1.5383176803588867\n",
      "epoch: 11, iteration: 1100, loss: 1.4846153259277344\n",
      "epoch: 11, iteration: 1200, loss: 1.483285903930664\n",
      "epoch: 11, iteration: 1300, loss: 1.5443133115768433\n",
      "epoch: 11, iteration: 1400, loss: 1.4788256883621216\n",
      "epoch: 11, iteration: 1500, loss: 1.48350191116333\n",
      "epoch: 11, iteration: 1600, loss: 1.6114095449447632\n",
      "epoch: 11, iteration: 1700, loss: 1.483272910118103\n",
      "epoch: 11, iteration: 1800, loss: 1.48343825340271\n",
      "epoch: 11, iteration: 1900, loss: 1.5104866027832031\n",
      "epoch: 11, iteration: 2000, loss: 1.4794071912765503\n",
      "epoch: 11, iteration: 2100, loss: 1.5322498083114624\n",
      "epoch: 11, iteration: 2200, loss: 1.6173607110977173\n",
      "epoch: 11, iteration: 2300, loss: 1.4899115562438965\n",
      "epoch: 11, iteration: 2400, loss: 1.5605642795562744\n",
      "epoch: 11, iteration: 2500, loss: 1.505218744277954\n",
      "epoch: 11, iteration: 2600, loss: 1.4852240085601807\n",
      "epoch: 11, iteration: 2700, loss: 1.567215085029602\n",
      "epoch: 11, iteration: 2800, loss: 1.475468397140503\n",
      "epoch: 11, iteration: 2900, loss: 1.4951701164245605\n",
      "epoch: 11, iteration: 3000, loss: 1.4932527542114258\n",
      "epoch: 11, iteration: 3100, loss: 1.4929448366165161\n",
      "epoch: 11, iteration: 3200, loss: 1.4885345697402954\n",
      "epoch: 11, iteration: 3300, loss: 1.5034998655319214\n",
      "epoch: 11, iteration: 3400, loss: 1.485353708267212\n",
      "epoch: 11, iteration: 3500, loss: 1.4852051734924316\n",
      "epoch: 11, iteration: 3600, loss: 1.4763672351837158\n",
      "epoch: 11, iteration: 3700, loss: 1.4860378503799438\n",
      "epoch: 11,train_accuracy:0.94745, test_accuracy: 0.9454\n",
      "epoch: 12, iteration: 0, loss: 1.4757323265075684\n",
      "epoch: 12, iteration: 100, loss: 1.53580904006958\n",
      "epoch: 12, iteration: 200, loss: 1.5349633693695068\n",
      "epoch: 12, iteration: 300, loss: 1.4867825508117676\n",
      "epoch: 12, iteration: 400, loss: 1.5370526313781738\n",
      "epoch: 12, iteration: 500, loss: 1.5056439638137817\n",
      "epoch: 12, iteration: 600, loss: 1.4977294206619263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, iteration: 700, loss: 1.4697402715682983\n",
      "epoch: 12, iteration: 800, loss: 1.5183398723602295\n",
      "epoch: 12, iteration: 900, loss: 1.4945354461669922\n",
      "epoch: 12, iteration: 1000, loss: 1.4695271253585815\n",
      "epoch: 12, iteration: 1100, loss: 1.5357122421264648\n",
      "epoch: 12, iteration: 1200, loss: 1.4987447261810303\n",
      "epoch: 12, iteration: 1300, loss: 1.4850021600723267\n",
      "epoch: 12, iteration: 1400, loss: 1.4754953384399414\n",
      "epoch: 12, iteration: 1500, loss: 1.482130765914917\n",
      "epoch: 12, iteration: 1600, loss: 1.5738236904144287\n",
      "epoch: 12, iteration: 1700, loss: 1.4734363555908203\n",
      "epoch: 12, iteration: 1800, loss: 1.4778988361358643\n",
      "epoch: 12, iteration: 1900, loss: 1.5118423700332642\n",
      "epoch: 12, iteration: 2000, loss: 1.4959505796432495\n",
      "epoch: 12, iteration: 2100, loss: 1.5292038917541504\n",
      "epoch: 12, iteration: 2200, loss: 1.5492767095565796\n",
      "epoch: 12, iteration: 2300, loss: 1.4896656274795532\n",
      "epoch: 12, iteration: 2400, loss: 1.5081311464309692\n",
      "epoch: 12, iteration: 2500, loss: 1.485548734664917\n",
      "epoch: 12, iteration: 2600, loss: 1.4880702495574951\n",
      "epoch: 12, iteration: 2700, loss: 1.6452035903930664\n",
      "epoch: 12, iteration: 2800, loss: 1.4944629669189453\n",
      "epoch: 12, iteration: 2900, loss: 1.5809348821640015\n",
      "epoch: 12, iteration: 3000, loss: 1.4737043380737305\n",
      "epoch: 12, iteration: 3100, loss: 1.522461175918579\n",
      "epoch: 12, iteration: 3200, loss: 1.4902499914169312\n",
      "epoch: 12, iteration: 3300, loss: 1.5196810960769653\n",
      "epoch: 12, iteration: 3400, loss: 1.5014137029647827\n",
      "epoch: 12, iteration: 3500, loss: 1.480059027671814\n",
      "epoch: 12, iteration: 3600, loss: 1.4810901880264282\n",
      "epoch: 12, iteration: 3700, loss: 1.4911901950836182\n",
      "epoch: 12,train_accuracy:0.94895, test_accuracy: 0.9468\n",
      "epoch: 13, iteration: 0, loss: 1.4835906028747559\n",
      "epoch: 13, iteration: 100, loss: 1.5433036088943481\n",
      "epoch: 13, iteration: 200, loss: 1.4761277437210083\n",
      "epoch: 13, iteration: 300, loss: 1.528756856918335\n",
      "epoch: 13, iteration: 400, loss: 1.4835454225540161\n",
      "epoch: 13, iteration: 500, loss: 1.5026259422302246\n",
      "epoch: 13, iteration: 600, loss: 1.5522069931030273\n",
      "epoch: 13, iteration: 700, loss: 1.536752462387085\n",
      "epoch: 13, iteration: 800, loss: 1.5205191373825073\n",
      "epoch: 13, iteration: 900, loss: 1.4781087636947632\n",
      "epoch: 13, iteration: 1000, loss: 1.5468792915344238\n",
      "epoch: 13, iteration: 1100, loss: 1.472891092300415\n",
      "epoch: 13, iteration: 1200, loss: 1.4693856239318848\n",
      "epoch: 13, iteration: 1300, loss: 1.4711718559265137\n",
      "epoch: 13, iteration: 1400, loss: 1.5599660873413086\n",
      "epoch: 13, iteration: 1500, loss: 1.5062865018844604\n",
      "epoch: 13, iteration: 1600, loss: 1.4750213623046875\n",
      "epoch: 13, iteration: 1700, loss: 1.498382568359375\n",
      "epoch: 13, iteration: 1800, loss: 1.524350881576538\n",
      "epoch: 13, iteration: 1900, loss: 1.4782402515411377\n",
      "epoch: 13, iteration: 2000, loss: 1.4860255718231201\n",
      "epoch: 13, iteration: 2100, loss: 1.480660080909729\n",
      "epoch: 13, iteration: 2200, loss: 1.4707845449447632\n",
      "epoch: 13, iteration: 2300, loss: 1.4823646545410156\n",
      "epoch: 13, iteration: 2400, loss: 1.5474849939346313\n",
      "epoch: 13, iteration: 2500, loss: 1.534127116203308\n",
      "epoch: 13, iteration: 2600, loss: 1.5281339883804321\n",
      "epoch: 13, iteration: 2700, loss: 1.4931540489196777\n",
      "epoch: 13, iteration: 2800, loss: 1.4838080406188965\n",
      "epoch: 13, iteration: 2900, loss: 1.5266051292419434\n",
      "epoch: 13, iteration: 3000, loss: 1.4681154489517212\n",
      "epoch: 13, iteration: 3100, loss: 1.6118965148925781\n",
      "epoch: 13, iteration: 3200, loss: 1.515156626701355\n",
      "epoch: 13, iteration: 3300, loss: 1.5241776704788208\n",
      "epoch: 13, iteration: 3400, loss: 1.4693818092346191\n",
      "epoch: 13, iteration: 3500, loss: 1.5378936529159546\n",
      "epoch: 13, iteration: 3600, loss: 1.5048648118972778\n",
      "epoch: 13, iteration: 3700, loss: 1.558289885520935\n",
      "epoch: 13,train_accuracy:0.95065, test_accuracy: 0.9485\n",
      "epoch: 14, iteration: 0, loss: 1.4802111387252808\n",
      "epoch: 14, iteration: 100, loss: 1.4716711044311523\n",
      "epoch: 14, iteration: 200, loss: 1.4849693775177002\n",
      "epoch: 14, iteration: 300, loss: 1.4699492454528809\n",
      "epoch: 14, iteration: 400, loss: 1.5173388719558716\n",
      "epoch: 14, iteration: 500, loss: 1.5428742170333862\n",
      "epoch: 14, iteration: 600, loss: 1.5264177322387695\n",
      "epoch: 14, iteration: 700, loss: 1.4758456945419312\n",
      "epoch: 14, iteration: 800, loss: 1.5477735996246338\n",
      "epoch: 14, iteration: 900, loss: 1.495873212814331\n",
      "epoch: 14, iteration: 1000, loss: 1.6161530017852783\n",
      "epoch: 14, iteration: 1100, loss: 1.59478759765625\n",
      "epoch: 14, iteration: 1200, loss: 1.4950652122497559\n",
      "epoch: 14, iteration: 1300, loss: 1.469537377357483\n",
      "epoch: 14, iteration: 1400, loss: 1.560392141342163\n",
      "epoch: 14, iteration: 1500, loss: 1.482235312461853\n",
      "epoch: 14, iteration: 1600, loss: 1.531962513923645\n",
      "epoch: 14, iteration: 1700, loss: 1.4830522537231445\n",
      "epoch: 14, iteration: 1800, loss: 1.4945234060287476\n",
      "epoch: 14, iteration: 1900, loss: 1.4952772855758667\n",
      "epoch: 14, iteration: 2000, loss: 1.471537470817566\n",
      "epoch: 14, iteration: 2100, loss: 1.5655522346496582\n",
      "epoch: 14, iteration: 2200, loss: 1.4993884563446045\n",
      "epoch: 14, iteration: 2300, loss: 1.4745144844055176\n",
      "epoch: 14, iteration: 2400, loss: 1.5237623453140259\n",
      "epoch: 14, iteration: 2500, loss: 1.5068928003311157\n",
      "epoch: 14, iteration: 2600, loss: 1.5359569787979126\n",
      "epoch: 14, iteration: 2700, loss: 1.4923515319824219\n",
      "epoch: 14, iteration: 2800, loss: 1.467970609664917\n",
      "epoch: 14, iteration: 2900, loss: 1.5697203874588013\n",
      "epoch: 14, iteration: 3000, loss: 1.5803022384643555\n",
      "epoch: 14, iteration: 3100, loss: 1.4627883434295654\n",
      "epoch: 14, iteration: 3200, loss: 1.4698892831802368\n",
      "epoch: 14, iteration: 3300, loss: 1.5330137014389038\n",
      "epoch: 14, iteration: 3400, loss: 1.5069568157196045\n",
      "epoch: 14, iteration: 3500, loss: 1.5154093503952026\n",
      "epoch: 14, iteration: 3600, loss: 1.499924898147583\n",
      "epoch: 14, iteration: 3700, loss: 1.5528498888015747\n",
      "epoch: 14,train_accuracy:0.9522333333333334, test_accuracy: 0.9498\n",
      "epoch: 15, iteration: 0, loss: 1.4829869270324707\n",
      "epoch: 15, iteration: 100, loss: 1.4774210453033447\n",
      "epoch: 15, iteration: 200, loss: 1.5744495391845703\n",
      "epoch: 15, iteration: 300, loss: 1.4961620569229126\n",
      "epoch: 15, iteration: 400, loss: 1.496139407157898\n",
      "epoch: 15, iteration: 500, loss: 1.465273380279541\n",
      "epoch: 15, iteration: 600, loss: 1.4772436618804932\n",
      "epoch: 15, iteration: 700, loss: 1.4771054983139038\n",
      "epoch: 15, iteration: 800, loss: 1.4770028591156006\n",
      "epoch: 15, iteration: 900, loss: 1.4678635597229004\n",
      "epoch: 15, iteration: 1000, loss: 1.5124362707138062\n",
      "epoch: 15, iteration: 1100, loss: 1.5269559621810913\n",
      "epoch: 15, iteration: 1200, loss: 1.4920299053192139\n",
      "epoch: 15, iteration: 1300, loss: 1.5113606452941895\n",
      "epoch: 15, iteration: 1400, loss: 1.5441561937332153\n",
      "epoch: 15, iteration: 1500, loss: 1.4683796167373657\n",
      "epoch: 15, iteration: 1600, loss: 1.4651384353637695\n",
      "epoch: 15, iteration: 1700, loss: 1.5987048149108887\n",
      "epoch: 15, iteration: 1800, loss: 1.4954581260681152\n",
      "epoch: 15, iteration: 1900, loss: 1.5910334587097168\n",
      "epoch: 15, iteration: 2000, loss: 1.4777511358261108\n",
      "epoch: 15, iteration: 2100, loss: 1.4826219081878662\n",
      "epoch: 15, iteration: 2200, loss: 1.475555181503296\n",
      "epoch: 15, iteration: 2300, loss: 1.4641340970993042\n",
      "epoch: 15, iteration: 2400, loss: 1.4809728860855103\n",
      "epoch: 15, iteration: 2500, loss: 1.4906202554702759\n",
      "epoch: 15, iteration: 2600, loss: 1.4765466451644897\n",
      "epoch: 15, iteration: 2700, loss: 1.4921653270721436\n",
      "epoch: 15, iteration: 2800, loss: 1.4908368587493896\n",
      "epoch: 15, iteration: 2900, loss: 1.487608790397644\n",
      "epoch: 15, iteration: 3000, loss: 1.4847640991210938\n",
      "epoch: 15, iteration: 3100, loss: 1.4767986536026\n",
      "epoch: 15, iteration: 3200, loss: 1.5433520078659058\n",
      "epoch: 15, iteration: 3300, loss: 1.553612232208252\n",
      "epoch: 15, iteration: 3400, loss: 1.4625108242034912\n",
      "epoch: 15, iteration: 3500, loss: 1.491299033164978\n",
      "epoch: 15, iteration: 3600, loss: 1.4758174419403076\n",
      "epoch: 15, iteration: 3700, loss: 1.5393534898757935\n",
      "epoch: 15,train_accuracy:0.95345, test_accuracy: 0.9502\n",
      "epoch: 16, iteration: 0, loss: 1.5123952627182007\n",
      "epoch: 16, iteration: 100, loss: 1.5381333827972412\n",
      "epoch: 16, iteration: 200, loss: 1.5057342052459717\n",
      "epoch: 16, iteration: 300, loss: 1.4774091243743896\n",
      "epoch: 16, iteration: 400, loss: 1.4940476417541504\n",
      "epoch: 16, iteration: 500, loss: 1.528907299041748\n",
      "epoch: 16, iteration: 600, loss: 1.601226806640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, iteration: 700, loss: 1.4997284412384033\n",
      "epoch: 16, iteration: 800, loss: 1.4957129955291748\n",
      "epoch: 16, iteration: 900, loss: 1.4657530784606934\n",
      "epoch: 16, iteration: 1000, loss: 1.488081932067871\n",
      "epoch: 16, iteration: 1100, loss: 1.485388994216919\n",
      "epoch: 16, iteration: 1200, loss: 1.4667720794677734\n",
      "epoch: 16, iteration: 1300, loss: 1.4695535898208618\n",
      "epoch: 16, iteration: 1400, loss: 1.536978006362915\n",
      "epoch: 16, iteration: 1500, loss: 1.5294662714004517\n",
      "epoch: 16, iteration: 1600, loss: 1.490044355392456\n",
      "epoch: 16, iteration: 1700, loss: 1.4994304180145264\n",
      "epoch: 16, iteration: 1800, loss: 1.5303500890731812\n",
      "epoch: 16, iteration: 1900, loss: 1.582245945930481\n",
      "epoch: 16, iteration: 2000, loss: 1.496181845664978\n",
      "epoch: 16, iteration: 2100, loss: 1.542197585105896\n",
      "epoch: 16, iteration: 2200, loss: 1.4953471422195435\n",
      "epoch: 16, iteration: 2300, loss: 1.5314489603042603\n",
      "epoch: 16, iteration: 2400, loss: 1.572211503982544\n",
      "epoch: 16, iteration: 2500, loss: 1.4973318576812744\n",
      "epoch: 16, iteration: 2600, loss: 1.4673501253128052\n",
      "epoch: 16, iteration: 2700, loss: 1.6025383472442627\n",
      "epoch: 16, iteration: 2800, loss: 1.5238518714904785\n",
      "epoch: 16, iteration: 2900, loss: 1.5284465551376343\n",
      "epoch: 16, iteration: 3000, loss: 1.4793314933776855\n",
      "epoch: 16, iteration: 3100, loss: 1.5268323421478271\n",
      "epoch: 16, iteration: 3200, loss: 1.4952081441879272\n",
      "epoch: 16, iteration: 3300, loss: 1.4726026058197021\n",
      "epoch: 16, iteration: 3400, loss: 1.5307772159576416\n",
      "epoch: 16, iteration: 3500, loss: 1.4787577390670776\n",
      "epoch: 16, iteration: 3600, loss: 1.5033117532730103\n",
      "epoch: 16, iteration: 3700, loss: 1.490249752998352\n",
      "epoch: 16,train_accuracy:0.9546333333333333, test_accuracy: 0.9524\n",
      "epoch: 17, iteration: 0, loss: 1.4716417789459229\n",
      "epoch: 17, iteration: 100, loss: 1.473698377609253\n",
      "epoch: 17, iteration: 200, loss: 1.4630405902862549\n",
      "epoch: 17, iteration: 300, loss: 1.5557652711868286\n",
      "epoch: 17, iteration: 400, loss: 1.483202338218689\n",
      "epoch: 17, iteration: 500, loss: 1.4876549243927002\n",
      "epoch: 17, iteration: 600, loss: 1.4675683975219727\n",
      "epoch: 17, iteration: 700, loss: 1.5796483755111694\n",
      "epoch: 17, iteration: 800, loss: 1.503936529159546\n",
      "epoch: 17, iteration: 900, loss: 1.5486915111541748\n",
      "epoch: 17, iteration: 1000, loss: 1.4739385843276978\n",
      "epoch: 17, iteration: 1100, loss: 1.5128810405731201\n",
      "epoch: 17, iteration: 1200, loss: 1.4716992378234863\n",
      "epoch: 17, iteration: 1300, loss: 1.4712390899658203\n",
      "epoch: 17, iteration: 1400, loss: 1.520611047744751\n",
      "epoch: 17, iteration: 1500, loss: 1.5242383480072021\n",
      "epoch: 17, iteration: 1600, loss: 1.4808040857315063\n",
      "epoch: 17, iteration: 1700, loss: 1.474332332611084\n",
      "epoch: 17, iteration: 1800, loss: 1.5155924558639526\n",
      "epoch: 17, iteration: 1900, loss: 1.4691506624221802\n",
      "epoch: 17, iteration: 2000, loss: 1.4699466228485107\n",
      "epoch: 17, iteration: 2100, loss: 1.4985811710357666\n",
      "epoch: 17, iteration: 2200, loss: 1.4686098098754883\n",
      "epoch: 17, iteration: 2300, loss: 1.5225715637207031\n",
      "epoch: 17, iteration: 2400, loss: 1.4793621301651\n",
      "epoch: 17, iteration: 2500, loss: 1.5043492317199707\n",
      "epoch: 17, iteration: 2600, loss: 1.5541749000549316\n",
      "epoch: 17, iteration: 2700, loss: 1.490541696548462\n",
      "epoch: 17, iteration: 2800, loss: 1.5276578664779663\n",
      "epoch: 17, iteration: 2900, loss: 1.5323646068572998\n",
      "epoch: 17, iteration: 3000, loss: 1.4707982540130615\n",
      "epoch: 17, iteration: 3100, loss: 1.5516085624694824\n",
      "epoch: 17, iteration: 3200, loss: 1.473015546798706\n",
      "epoch: 17, iteration: 3300, loss: 1.5227240324020386\n",
      "epoch: 17, iteration: 3400, loss: 1.4926650524139404\n",
      "epoch: 17, iteration: 3500, loss: 1.4966222047805786\n",
      "epoch: 17, iteration: 3600, loss: 1.4853475093841553\n",
      "epoch: 17, iteration: 3700, loss: 1.4780056476593018\n",
      "epoch: 17,train_accuracy:0.9558666666666666, test_accuracy: 0.9528\n",
      "epoch: 18, iteration: 0, loss: 1.4715150594711304\n",
      "epoch: 18, iteration: 100, loss: 1.4625566005706787\n",
      "epoch: 18, iteration: 200, loss: 1.4788438081741333\n",
      "epoch: 18, iteration: 300, loss: 1.4674149751663208\n",
      "epoch: 18, iteration: 400, loss: 1.480868935585022\n",
      "epoch: 18, iteration: 500, loss: 1.4795037508010864\n",
      "epoch: 18, iteration: 600, loss: 1.502707839012146\n",
      "epoch: 18, iteration: 700, loss: 1.541917085647583\n",
      "epoch: 18, iteration: 800, loss: 1.477112889289856\n",
      "epoch: 18, iteration: 900, loss: 1.536712884902954\n",
      "epoch: 18, iteration: 1000, loss: 1.4722007513046265\n",
      "epoch: 18, iteration: 1100, loss: 1.4670193195343018\n",
      "epoch: 18, iteration: 1200, loss: 1.4780480861663818\n",
      "epoch: 18, iteration: 1300, loss: 1.5832386016845703\n",
      "epoch: 18, iteration: 1400, loss: 1.4826622009277344\n",
      "epoch: 18, iteration: 1500, loss: 1.4890644550323486\n",
      "epoch: 18, iteration: 1600, loss: 1.4775644540786743\n",
      "epoch: 18, iteration: 1700, loss: 1.4750036001205444\n",
      "epoch: 18, iteration: 1800, loss: 1.499110460281372\n",
      "epoch: 18, iteration: 1900, loss: 1.5791212320327759\n",
      "epoch: 18, iteration: 2000, loss: 1.470607876777649\n",
      "epoch: 18, iteration: 2100, loss: 1.4731817245483398\n",
      "epoch: 18, iteration: 2200, loss: 1.4951835870742798\n",
      "epoch: 18, iteration: 2300, loss: 1.6376385688781738\n",
      "epoch: 18, iteration: 2400, loss: 1.470432996749878\n",
      "epoch: 18, iteration: 2500, loss: 1.4690117835998535\n",
      "epoch: 18, iteration: 2600, loss: 1.4963786602020264\n",
      "epoch: 18, iteration: 2700, loss: 1.4795597791671753\n",
      "epoch: 18, iteration: 2800, loss: 1.4838463068008423\n",
      "epoch: 18, iteration: 2900, loss: 1.5281275510787964\n",
      "epoch: 18, iteration: 3000, loss: 1.4740148782730103\n",
      "epoch: 18, iteration: 3100, loss: 1.4896186590194702\n",
      "epoch: 18, iteration: 3200, loss: 1.4749999046325684\n",
      "epoch: 18, iteration: 3300, loss: 1.6035641431808472\n",
      "epoch: 18, iteration: 3400, loss: 1.4782830476760864\n",
      "epoch: 18, iteration: 3500, loss: 1.5510162115097046\n",
      "epoch: 18, iteration: 3600, loss: 1.5322257280349731\n",
      "epoch: 18, iteration: 3700, loss: 1.4909709692001343\n",
      "epoch: 18,train_accuracy:0.9574333333333334, test_accuracy: 0.9533\n",
      "epoch: 19, iteration: 0, loss: 1.484426736831665\n",
      "epoch: 19, iteration: 100, loss: 1.4721347093582153\n",
      "epoch: 19, iteration: 200, loss: 1.4761638641357422\n",
      "epoch: 19, iteration: 300, loss: 1.4859412908554077\n",
      "epoch: 19, iteration: 400, loss: 1.5010956525802612\n",
      "epoch: 19, iteration: 500, loss: 1.542785882949829\n",
      "epoch: 19, iteration: 600, loss: 1.5238401889801025\n",
      "epoch: 19, iteration: 700, loss: 1.485049843788147\n",
      "epoch: 19, iteration: 800, loss: 1.550344705581665\n",
      "epoch: 19, iteration: 900, loss: 1.4768664836883545\n",
      "epoch: 19, iteration: 1000, loss: 1.5238069295883179\n",
      "epoch: 19, iteration: 1100, loss: 1.484421968460083\n",
      "epoch: 19, iteration: 1200, loss: 1.5267404317855835\n",
      "epoch: 19, iteration: 1300, loss: 1.480272889137268\n",
      "epoch: 19, iteration: 1400, loss: 1.490817904472351\n",
      "epoch: 19, iteration: 1500, loss: 1.474967360496521\n",
      "epoch: 19, iteration: 1600, loss: 1.4659357070922852\n",
      "epoch: 19, iteration: 1700, loss: 1.5409609079360962\n",
      "epoch: 19, iteration: 1800, loss: 1.4807883501052856\n",
      "epoch: 19, iteration: 1900, loss: 1.50022554397583\n",
      "epoch: 19, iteration: 2000, loss: 1.5342248678207397\n",
      "epoch: 19, iteration: 2100, loss: 1.4708061218261719\n",
      "epoch: 19, iteration: 2200, loss: 1.4836194515228271\n",
      "epoch: 19, iteration: 2300, loss: 1.4735556840896606\n",
      "epoch: 19, iteration: 2400, loss: 1.5018885135650635\n",
      "epoch: 19, iteration: 2500, loss: 1.4720040559768677\n",
      "epoch: 19, iteration: 2600, loss: 1.4902280569076538\n",
      "epoch: 19, iteration: 2700, loss: 1.4867340326309204\n",
      "epoch: 19, iteration: 2800, loss: 1.4845056533813477\n",
      "epoch: 19, iteration: 2900, loss: 1.4808883666992188\n",
      "epoch: 19, iteration: 3000, loss: 1.468999981880188\n",
      "epoch: 19, iteration: 3100, loss: 1.6194140911102295\n",
      "epoch: 19, iteration: 3200, loss: 1.5280163288116455\n",
      "epoch: 19, iteration: 3300, loss: 1.5412602424621582\n",
      "epoch: 19, iteration: 3400, loss: 1.4854851961135864\n",
      "epoch: 19, iteration: 3500, loss: 1.5450637340545654\n",
      "epoch: 19, iteration: 3600, loss: 1.5239853858947754\n",
      "epoch: 19, iteration: 3700, loss: 1.5066813230514526\n",
      "epoch: 19,train_accuracy:0.9587, test_accuracy: 0.9541\n",
      "epoch: 20, iteration: 0, loss: 1.4785903692245483\n",
      "epoch: 20, iteration: 100, loss: 1.4787659645080566\n",
      "epoch: 20, iteration: 200, loss: 1.4706474542617798\n",
      "epoch: 20, iteration: 300, loss: 1.5343458652496338\n",
      "epoch: 20, iteration: 400, loss: 1.483848214149475\n",
      "epoch: 20, iteration: 500, loss: 1.4780267477035522\n",
      "epoch: 20, iteration: 600, loss: 1.517077922821045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, iteration: 700, loss: 1.5243065357208252\n",
      "epoch: 20, iteration: 800, loss: 1.4767998456954956\n",
      "epoch: 20, iteration: 900, loss: 1.4652172327041626\n",
      "epoch: 20, iteration: 1000, loss: 1.4890440702438354\n",
      "epoch: 20, iteration: 1100, loss: 1.4852677583694458\n",
      "epoch: 20, iteration: 1200, loss: 1.4764049053192139\n",
      "epoch: 20, iteration: 1300, loss: 1.4789488315582275\n",
      "epoch: 20, iteration: 1400, loss: 1.4846004247665405\n",
      "epoch: 20, iteration: 1500, loss: 1.4653890132904053\n",
      "epoch: 20, iteration: 1600, loss: 1.521804690361023\n",
      "epoch: 20, iteration: 1700, loss: 1.4683372974395752\n",
      "epoch: 20, iteration: 1800, loss: 1.5395863056182861\n",
      "epoch: 20, iteration: 1900, loss: 1.4701874256134033\n",
      "epoch: 20, iteration: 2000, loss: 1.4697623252868652\n",
      "epoch: 20, iteration: 2100, loss: 1.4840266704559326\n",
      "epoch: 20, iteration: 2200, loss: 1.5456137657165527\n",
      "epoch: 20, iteration: 2300, loss: 1.4741804599761963\n",
      "epoch: 20, iteration: 2400, loss: 1.520512580871582\n",
      "epoch: 20, iteration: 2500, loss: 1.470987319946289\n",
      "epoch: 20, iteration: 2600, loss: 1.4872112274169922\n",
      "epoch: 20, iteration: 2700, loss: 1.4741907119750977\n",
      "epoch: 20, iteration: 2800, loss: 1.491421103477478\n",
      "epoch: 20, iteration: 2900, loss: 1.5309183597564697\n",
      "epoch: 20, iteration: 3000, loss: 1.4951318502426147\n",
      "epoch: 20, iteration: 3100, loss: 1.470180630683899\n",
      "epoch: 20, iteration: 3200, loss: 1.4728589057922363\n",
      "epoch: 20, iteration: 3300, loss: 1.592933177947998\n",
      "epoch: 20, iteration: 3400, loss: 1.5516891479492188\n",
      "epoch: 20, iteration: 3500, loss: 1.5240478515625\n",
      "epoch: 20, iteration: 3600, loss: 1.4926390647888184\n",
      "epoch: 20, iteration: 3700, loss: 1.554787516593933\n",
      "epoch: 20,train_accuracy:0.9596666666666667, test_accuracy: 0.9552\n",
      "epoch: 21, iteration: 0, loss: 1.5274839401245117\n",
      "epoch: 21, iteration: 100, loss: 1.4746997356414795\n",
      "epoch: 21, iteration: 200, loss: 1.4684571027755737\n",
      "epoch: 21, iteration: 300, loss: 1.4630697965621948\n",
      "epoch: 21, iteration: 400, loss: 1.5478824377059937\n",
      "epoch: 21, iteration: 500, loss: 1.4818496704101562\n",
      "epoch: 21, iteration: 600, loss: 1.4713248014450073\n",
      "epoch: 21, iteration: 700, loss: 1.50371253490448\n",
      "epoch: 21, iteration: 800, loss: 1.474652886390686\n",
      "epoch: 21, iteration: 900, loss: 1.4654419422149658\n",
      "epoch: 21, iteration: 1000, loss: 1.4942152500152588\n",
      "epoch: 21, iteration: 1100, loss: 1.548627495765686\n",
      "epoch: 21, iteration: 1200, loss: 1.475272297859192\n",
      "epoch: 21, iteration: 1300, loss: 1.4793325662612915\n",
      "epoch: 21, iteration: 1400, loss: 1.4766477346420288\n",
      "epoch: 21, iteration: 1500, loss: 1.4643934965133667\n",
      "epoch: 21, iteration: 1600, loss: 1.550714373588562\n",
      "epoch: 21, iteration: 1700, loss: 1.4992681741714478\n",
      "epoch: 21, iteration: 1800, loss: 1.5250722169876099\n",
      "epoch: 21, iteration: 1900, loss: 1.4900414943695068\n",
      "epoch: 21, iteration: 2000, loss: 1.5287857055664062\n",
      "epoch: 21, iteration: 2100, loss: 1.5027393102645874\n",
      "epoch: 21, iteration: 2200, loss: 1.4815120697021484\n",
      "epoch: 21, iteration: 2300, loss: 1.4775643348693848\n",
      "epoch: 21, iteration: 2400, loss: 1.5725712776184082\n",
      "epoch: 21, iteration: 2500, loss: 1.4950755834579468\n",
      "epoch: 21, iteration: 2600, loss: 1.4631173610687256\n",
      "epoch: 21, iteration: 2700, loss: 1.4629096984863281\n",
      "epoch: 21, iteration: 2800, loss: 1.5291683673858643\n",
      "epoch: 21, iteration: 2900, loss: 1.488925814628601\n",
      "epoch: 21, iteration: 3000, loss: 1.5495957136154175\n",
      "epoch: 21, iteration: 3100, loss: 1.4799422025680542\n",
      "epoch: 21, iteration: 3200, loss: 1.642620325088501\n",
      "epoch: 21, iteration: 3300, loss: 1.4764339923858643\n",
      "epoch: 21, iteration: 3400, loss: 1.482279658317566\n",
      "epoch: 21, iteration: 3500, loss: 1.4681792259216309\n",
      "epoch: 21, iteration: 3600, loss: 1.5438143014907837\n",
      "epoch: 21, iteration: 3700, loss: 1.474704623222351\n",
      "epoch: 21,train_accuracy:0.9603666666666667, test_accuracy: 0.9555\n",
      "epoch: 22, iteration: 0, loss: 1.486632227897644\n",
      "epoch: 22, iteration: 100, loss: 1.5362074375152588\n",
      "epoch: 22, iteration: 200, loss: 1.471781849861145\n",
      "epoch: 22, iteration: 300, loss: 1.5387221574783325\n",
      "epoch: 22, iteration: 400, loss: 1.4800244569778442\n",
      "epoch: 22, iteration: 500, loss: 1.4797964096069336\n",
      "epoch: 22, iteration: 600, loss: 1.4680413007736206\n",
      "epoch: 22, iteration: 700, loss: 1.5316104888916016\n",
      "epoch: 22, iteration: 800, loss: 1.4885059595108032\n",
      "epoch: 22, iteration: 900, loss: 1.4728935956954956\n",
      "epoch: 22, iteration: 1000, loss: 1.4621050357818604\n",
      "epoch: 22, iteration: 1100, loss: 1.4673703908920288\n",
      "epoch: 22, iteration: 1200, loss: 1.4768860340118408\n",
      "epoch: 22, iteration: 1300, loss: 1.4817088842391968\n",
      "epoch: 22, iteration: 1400, loss: 1.4835666418075562\n",
      "epoch: 22, iteration: 1500, loss: 1.4736701250076294\n",
      "epoch: 22, iteration: 1600, loss: 1.4642395973205566\n",
      "epoch: 22, iteration: 1700, loss: 1.492506742477417\n",
      "epoch: 22, iteration: 1800, loss: 1.4784003496170044\n",
      "epoch: 22, iteration: 1900, loss: 1.4664524793624878\n",
      "epoch: 22, iteration: 2000, loss: 1.4808096885681152\n",
      "epoch: 22, iteration: 2100, loss: 1.4712741374969482\n",
      "epoch: 22, iteration: 2200, loss: 1.4808710813522339\n",
      "epoch: 22, iteration: 2300, loss: 1.5831027030944824\n",
      "epoch: 22, iteration: 2400, loss: 1.4869002103805542\n",
      "epoch: 22, iteration: 2500, loss: 1.5254038572311401\n",
      "epoch: 22, iteration: 2600, loss: 1.4769335985183716\n",
      "epoch: 22, iteration: 2700, loss: 1.5335485935211182\n",
      "epoch: 22, iteration: 2800, loss: 1.4722684621810913\n",
      "epoch: 22, iteration: 2900, loss: 1.5119373798370361\n",
      "epoch: 22, iteration: 3000, loss: 1.480092167854309\n",
      "epoch: 22, iteration: 3100, loss: 1.5351194143295288\n",
      "epoch: 22, iteration: 3200, loss: 1.4661507606506348\n",
      "epoch: 22, iteration: 3300, loss: 1.5337657928466797\n",
      "epoch: 22, iteration: 3400, loss: 1.4726587533950806\n",
      "epoch: 22, iteration: 3500, loss: 1.466455340385437\n",
      "epoch: 22, iteration: 3600, loss: 1.6242369413375854\n",
      "epoch: 22, iteration: 3700, loss: 1.46758234500885\n",
      "epoch: 22,train_accuracy:0.9614833333333334, test_accuracy: 0.9569\n",
      "epoch: 23, iteration: 0, loss: 1.4688421487808228\n",
      "epoch: 23, iteration: 100, loss: 1.4710522890090942\n",
      "epoch: 23, iteration: 200, loss: 1.4832346439361572\n",
      "epoch: 23, iteration: 300, loss: 1.489222764968872\n",
      "epoch: 23, iteration: 400, loss: 1.4645251035690308\n",
      "epoch: 23, iteration: 500, loss: 1.540450930595398\n",
      "epoch: 23, iteration: 600, loss: 1.485088586807251\n",
      "epoch: 23, iteration: 700, loss: 1.6619961261749268\n",
      "epoch: 23, iteration: 800, loss: 1.5363460779190063\n",
      "epoch: 23, iteration: 900, loss: 1.4773938655853271\n",
      "epoch: 23, iteration: 1000, loss: 1.468978762626648\n",
      "epoch: 23, iteration: 1100, loss: 1.5084387063980103\n",
      "epoch: 23, iteration: 1200, loss: 1.5019500255584717\n",
      "epoch: 23, iteration: 1300, loss: 1.4633140563964844\n",
      "epoch: 23, iteration: 1400, loss: 1.5037639141082764\n",
      "epoch: 23, iteration: 1500, loss: 1.4690405130386353\n",
      "epoch: 23, iteration: 1600, loss: 1.486585021018982\n",
      "epoch: 23, iteration: 1700, loss: 1.4943746328353882\n",
      "epoch: 23, iteration: 1800, loss: 1.4795503616333008\n",
      "epoch: 23, iteration: 1900, loss: 1.5444971323013306\n",
      "epoch: 23, iteration: 2000, loss: 1.5187983512878418\n",
      "epoch: 23, iteration: 2100, loss: 1.4826782941818237\n",
      "epoch: 23, iteration: 2200, loss: 1.4864779710769653\n",
      "epoch: 23, iteration: 2300, loss: 1.4791369438171387\n",
      "epoch: 23, iteration: 2400, loss: 1.5281764268875122\n",
      "epoch: 23, iteration: 2500, loss: 1.4796704053878784\n",
      "epoch: 23, iteration: 2600, loss: 1.4832510948181152\n",
      "epoch: 23, iteration: 2700, loss: 1.4653630256652832\n",
      "epoch: 23, iteration: 2800, loss: 1.4846913814544678\n",
      "epoch: 23, iteration: 2900, loss: 1.477067470550537\n",
      "epoch: 23, iteration: 3000, loss: 1.4684536457061768\n",
      "epoch: 23, iteration: 3100, loss: 1.5049480199813843\n",
      "epoch: 23, iteration: 3200, loss: 1.500831961631775\n",
      "epoch: 23, iteration: 3300, loss: 1.5347299575805664\n",
      "epoch: 23, iteration: 3400, loss: 1.5289353132247925\n",
      "epoch: 23, iteration: 3500, loss: 1.470936894416809\n",
      "epoch: 23, iteration: 3600, loss: 1.4687473773956299\n",
      "epoch: 23, iteration: 3700, loss: 1.539486289024353\n",
      "epoch: 23,train_accuracy:0.96225, test_accuracy: 0.9571\n",
      "epoch: 24, iteration: 0, loss: 1.4808375835418701\n",
      "epoch: 24, iteration: 100, loss: 1.4795013666152954\n",
      "epoch: 24, iteration: 200, loss: 1.4862747192382812\n",
      "epoch: 24, iteration: 300, loss: 1.4641433954238892\n",
      "epoch: 24, iteration: 400, loss: 1.5432612895965576\n",
      "epoch: 24, iteration: 500, loss: 1.482845425605774\n",
      "epoch: 24, iteration: 600, loss: 1.4631868600845337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24, iteration: 700, loss: 1.4707021713256836\n",
      "epoch: 24, iteration: 800, loss: 1.5473556518554688\n",
      "epoch: 24, iteration: 900, loss: 1.4933174848556519\n",
      "epoch: 24, iteration: 1000, loss: 1.4751204252243042\n",
      "epoch: 24, iteration: 1100, loss: 1.4831780195236206\n",
      "epoch: 24, iteration: 1200, loss: 1.472456932067871\n",
      "epoch: 24, iteration: 1300, loss: 1.4739130735397339\n",
      "epoch: 24, iteration: 1400, loss: 1.4826740026474\n",
      "epoch: 24, iteration: 1500, loss: 1.618055820465088\n",
      "epoch: 24, iteration: 1600, loss: 1.5418388843536377\n",
      "epoch: 24, iteration: 1700, loss: 1.472023367881775\n",
      "epoch: 24, iteration: 1800, loss: 1.463981032371521\n",
      "epoch: 24, iteration: 1900, loss: 1.470383882522583\n",
      "epoch: 24, iteration: 2000, loss: 1.4818694591522217\n",
      "epoch: 24, iteration: 2100, loss: 1.5414979457855225\n",
      "epoch: 24, iteration: 2200, loss: 1.4765856266021729\n",
      "epoch: 24, iteration: 2300, loss: 1.4980442523956299\n",
      "epoch: 24, iteration: 2400, loss: 1.560036540031433\n",
      "epoch: 24, iteration: 2500, loss: 1.46486234664917\n",
      "epoch: 24, iteration: 2600, loss: 1.5325822830200195\n",
      "epoch: 24, iteration: 2700, loss: 1.5477657318115234\n",
      "epoch: 24, iteration: 2800, loss: 1.4702996015548706\n",
      "epoch: 24, iteration: 2900, loss: 1.4769953489303589\n",
      "epoch: 24, iteration: 3000, loss: 1.4863479137420654\n",
      "epoch: 24, iteration: 3100, loss: 1.4854228496551514\n",
      "epoch: 24, iteration: 3200, loss: 1.532261848449707\n",
      "epoch: 24, iteration: 3300, loss: 1.5258378982543945\n",
      "epoch: 24, iteration: 3400, loss: 1.533644437789917\n",
      "epoch: 24, iteration: 3500, loss: 1.4651577472686768\n",
      "epoch: 24, iteration: 3600, loss: 1.4763680696487427\n",
      "epoch: 24, iteration: 3700, loss: 1.4827057123184204\n",
      "epoch: 24,train_accuracy:0.9631166666666666, test_accuracy: 0.9571\n",
      "epoch: 25, iteration: 0, loss: 1.5057356357574463\n",
      "epoch: 25, iteration: 100, loss: 1.538523554801941\n",
      "epoch: 25, iteration: 200, loss: 1.5003993511199951\n",
      "epoch: 25, iteration: 300, loss: 1.4788209199905396\n",
      "epoch: 25, iteration: 400, loss: 1.46747624874115\n",
      "epoch: 25, iteration: 500, loss: 1.5049914121627808\n",
      "epoch: 25, iteration: 600, loss: 1.4749892950057983\n",
      "epoch: 25, iteration: 700, loss: 1.4890998601913452\n",
      "epoch: 25, iteration: 800, loss: 1.4834121465682983\n",
      "epoch: 25, iteration: 900, loss: 1.4798355102539062\n",
      "epoch: 25, iteration: 1000, loss: 1.475406289100647\n",
      "epoch: 25, iteration: 1100, loss: 1.4731327295303345\n",
      "epoch: 25, iteration: 1200, loss: 1.4710936546325684\n",
      "epoch: 25, iteration: 1300, loss: 1.4773997068405151\n",
      "epoch: 25, iteration: 1400, loss: 1.46661376953125\n",
      "epoch: 25, iteration: 1500, loss: 1.4707986116409302\n",
      "epoch: 25, iteration: 1600, loss: 1.4722270965576172\n",
      "epoch: 25, iteration: 1700, loss: 1.4748036861419678\n",
      "epoch: 25, iteration: 1800, loss: 1.5560250282287598\n",
      "epoch: 25, iteration: 1900, loss: 1.462530255317688\n",
      "epoch: 25, iteration: 2000, loss: 1.5986957550048828\n",
      "epoch: 25, iteration: 2100, loss: 1.487402319908142\n",
      "epoch: 25, iteration: 2200, loss: 1.46332585811615\n",
      "epoch: 25, iteration: 2300, loss: 1.4952195882797241\n",
      "epoch: 25, iteration: 2400, loss: 1.467290997505188\n",
      "epoch: 25, iteration: 2500, loss: 1.4641339778900146\n",
      "epoch: 25, iteration: 2600, loss: 1.4698007106781006\n",
      "epoch: 25, iteration: 2700, loss: 1.5024694204330444\n",
      "epoch: 25, iteration: 2800, loss: 1.4673774242401123\n",
      "epoch: 25, iteration: 2900, loss: 1.467640995979309\n",
      "epoch: 25, iteration: 3000, loss: 1.5227853059768677\n",
      "epoch: 25, iteration: 3100, loss: 1.542481780052185\n",
      "epoch: 25, iteration: 3200, loss: 1.4703774452209473\n",
      "epoch: 25, iteration: 3300, loss: 1.498075246810913\n",
      "epoch: 25, iteration: 3400, loss: 1.478238582611084\n",
      "epoch: 25, iteration: 3500, loss: 1.474838137626648\n",
      "epoch: 25, iteration: 3600, loss: 1.503732681274414\n",
      "epoch: 25, iteration: 3700, loss: 1.4678839445114136\n",
      "epoch: 25,train_accuracy:0.96365, test_accuracy: 0.9594\n",
      "epoch: 26, iteration: 0, loss: 1.5001025199890137\n",
      "epoch: 26, iteration: 100, loss: 1.4777185916900635\n",
      "epoch: 26, iteration: 200, loss: 1.5408984422683716\n",
      "epoch: 26, iteration: 300, loss: 1.4726380109786987\n",
      "epoch: 26, iteration: 400, loss: 1.4824354648590088\n",
      "epoch: 26, iteration: 500, loss: 1.4679657220840454\n",
      "epoch: 26, iteration: 600, loss: 1.483237385749817\n",
      "epoch: 26, iteration: 700, loss: 1.5090892314910889\n",
      "epoch: 26, iteration: 800, loss: 1.4709458351135254\n",
      "epoch: 26, iteration: 900, loss: 1.4808058738708496\n",
      "epoch: 26, iteration: 1000, loss: 1.4830251932144165\n",
      "epoch: 26, iteration: 1100, loss: 1.4700723886489868\n",
      "epoch: 26, iteration: 1200, loss: 1.4725126028060913\n",
      "epoch: 26, iteration: 1300, loss: 1.4786616563796997\n",
      "epoch: 26, iteration: 1400, loss: 1.5372487306594849\n",
      "epoch: 26, iteration: 1500, loss: 1.4680441617965698\n",
      "epoch: 26, iteration: 1600, loss: 1.5112539529800415\n",
      "epoch: 26, iteration: 1700, loss: 1.4733457565307617\n",
      "epoch: 26, iteration: 1800, loss: 1.517128825187683\n",
      "epoch: 26, iteration: 1900, loss: 1.4624528884887695\n",
      "epoch: 26, iteration: 2000, loss: 1.4683711528778076\n",
      "epoch: 26, iteration: 2100, loss: 1.5806593894958496\n",
      "epoch: 26, iteration: 2200, loss: 1.4844589233398438\n",
      "epoch: 26, iteration: 2300, loss: 1.4733831882476807\n",
      "epoch: 26, iteration: 2400, loss: 1.4895763397216797\n",
      "epoch: 26, iteration: 2500, loss: 1.4774329662322998\n",
      "epoch: 26, iteration: 2600, loss: 1.4729299545288086\n",
      "epoch: 26, iteration: 2700, loss: 1.4863346815109253\n",
      "epoch: 26, iteration: 2800, loss: 1.521628499031067\n",
      "epoch: 26, iteration: 2900, loss: 1.4862865209579468\n",
      "epoch: 26, iteration: 3000, loss: 1.4682457447052002\n",
      "epoch: 26, iteration: 3100, loss: 1.4732787609100342\n",
      "epoch: 26, iteration: 3200, loss: 1.4715858697891235\n",
      "epoch: 26, iteration: 3300, loss: 1.4886189699172974\n",
      "epoch: 26, iteration: 3400, loss: 1.528225064277649\n",
      "epoch: 26, iteration: 3500, loss: 1.4672127962112427\n",
      "epoch: 26, iteration: 3600, loss: 1.539790153503418\n",
      "epoch: 26, iteration: 3700, loss: 1.4688642024993896\n",
      "epoch: 26,train_accuracy:0.9644666666666667, test_accuracy: 0.9588\n",
      "epoch: 27, iteration: 0, loss: 1.491301417350769\n",
      "epoch: 27, iteration: 100, loss: 1.4681018590927124\n",
      "epoch: 27, iteration: 200, loss: 1.5242395401000977\n",
      "epoch: 27, iteration: 300, loss: 1.5433757305145264\n",
      "epoch: 27, iteration: 400, loss: 1.4619296789169312\n",
      "epoch: 27, iteration: 500, loss: 1.475630283355713\n",
      "epoch: 27, iteration: 600, loss: 1.47149658203125\n",
      "epoch: 27, iteration: 700, loss: 1.532477617263794\n",
      "epoch: 27, iteration: 800, loss: 1.5584722757339478\n",
      "epoch: 27, iteration: 900, loss: 1.463197946548462\n",
      "epoch: 27, iteration: 1000, loss: 1.4666944742202759\n",
      "epoch: 27, iteration: 1100, loss: 1.4686791896820068\n",
      "epoch: 27, iteration: 1200, loss: 1.4971309900283813\n",
      "epoch: 27, iteration: 1300, loss: 1.4752495288848877\n",
      "epoch: 27, iteration: 1400, loss: 1.5031051635742188\n",
      "epoch: 27, iteration: 1500, loss: 1.4665288925170898\n",
      "epoch: 27, iteration: 1600, loss: 1.5237987041473389\n",
      "epoch: 27, iteration: 1700, loss: 1.4777895212173462\n",
      "epoch: 27, iteration: 1800, loss: 1.5384551286697388\n",
      "epoch: 27, iteration: 1900, loss: 1.4659463167190552\n",
      "epoch: 27, iteration: 2000, loss: 1.474482774734497\n",
      "epoch: 27, iteration: 2100, loss: 1.5338466167449951\n",
      "epoch: 27, iteration: 2200, loss: 1.4826805591583252\n",
      "epoch: 27, iteration: 2300, loss: 1.5011078119277954\n",
      "epoch: 27, iteration: 2400, loss: 1.4618828296661377\n",
      "epoch: 27, iteration: 2500, loss: 1.4738872051239014\n",
      "epoch: 27, iteration: 2600, loss: 1.551996111869812\n",
      "epoch: 27, iteration: 2700, loss: 1.4727954864501953\n",
      "epoch: 27, iteration: 2800, loss: 1.468799352645874\n",
      "epoch: 27, iteration: 2900, loss: 1.5155357122421265\n",
      "epoch: 27, iteration: 3000, loss: 1.4857103824615479\n",
      "epoch: 27, iteration: 3100, loss: 1.4744442701339722\n",
      "epoch: 27, iteration: 3200, loss: 1.4935239553451538\n",
      "epoch: 27, iteration: 3300, loss: 1.4706107378005981\n",
      "epoch: 27, iteration: 3400, loss: 1.5046600103378296\n",
      "epoch: 27, iteration: 3500, loss: 1.531860589981079\n",
      "epoch: 27, iteration: 3600, loss: 1.467699646949768\n",
      "epoch: 27, iteration: 3700, loss: 1.5147229433059692\n",
      "epoch: 27,train_accuracy:0.96525, test_accuracy: 0.9601\n",
      "epoch: 28, iteration: 0, loss: 1.5638493299484253\n",
      "epoch: 28, iteration: 100, loss: 1.5864431858062744\n",
      "epoch: 28, iteration: 200, loss: 1.4738669395446777\n",
      "epoch: 28, iteration: 300, loss: 1.4792360067367554\n",
      "epoch: 28, iteration: 400, loss: 1.5272502899169922\n",
      "epoch: 28, iteration: 500, loss: 1.4873963594436646\n",
      "epoch: 28, iteration: 600, loss: 1.4677587747573853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28, iteration: 700, loss: 1.4726580381393433\n",
      "epoch: 28, iteration: 800, loss: 1.4714781045913696\n",
      "epoch: 28, iteration: 900, loss: 1.5785273313522339\n",
      "epoch: 28, iteration: 1000, loss: 1.5142937898635864\n",
      "epoch: 28, iteration: 1100, loss: 1.479032278060913\n",
      "epoch: 28, iteration: 1200, loss: 1.539494514465332\n",
      "epoch: 28, iteration: 1300, loss: 1.550016164779663\n",
      "epoch: 28, iteration: 1400, loss: 1.5034728050231934\n",
      "epoch: 28, iteration: 1500, loss: 1.487766981124878\n",
      "epoch: 28, iteration: 1600, loss: 1.4711360931396484\n",
      "epoch: 28, iteration: 1700, loss: 1.475306510925293\n",
      "epoch: 28, iteration: 1800, loss: 1.471225619316101\n",
      "epoch: 28, iteration: 1900, loss: 1.4795222282409668\n",
      "epoch: 28, iteration: 2000, loss: 1.4849945306777954\n",
      "epoch: 28, iteration: 2100, loss: 1.4654183387756348\n",
      "epoch: 28, iteration: 2200, loss: 1.4668537378311157\n",
      "epoch: 28, iteration: 2300, loss: 1.4672447443008423\n",
      "epoch: 28, iteration: 2400, loss: 1.4822684526443481\n",
      "epoch: 28, iteration: 2500, loss: 1.466929316520691\n",
      "epoch: 28, iteration: 2600, loss: 1.4665043354034424\n",
      "epoch: 28, iteration: 2700, loss: 1.4681521654129028\n",
      "epoch: 28, iteration: 2800, loss: 1.4730535745620728\n",
      "epoch: 28, iteration: 2900, loss: 1.4667243957519531\n",
      "epoch: 28, iteration: 3000, loss: 1.5756992101669312\n",
      "epoch: 28, iteration: 3100, loss: 1.5219900608062744\n",
      "epoch: 28, iteration: 3200, loss: 1.4651304483413696\n",
      "epoch: 28, iteration: 3300, loss: 1.502079963684082\n",
      "epoch: 28, iteration: 3400, loss: 1.470264196395874\n",
      "epoch: 28, iteration: 3500, loss: 1.4878379106521606\n",
      "epoch: 28, iteration: 3600, loss: 1.4675744771957397\n",
      "epoch: 28, iteration: 3700, loss: 1.6151065826416016\n",
      "epoch: 28,train_accuracy:0.9663166666666667, test_accuracy: 0.9601\n",
      "epoch: 29, iteration: 0, loss: 1.4656933546066284\n",
      "epoch: 29, iteration: 100, loss: 1.462834358215332\n",
      "epoch: 29, iteration: 200, loss: 1.4689385890960693\n",
      "epoch: 29, iteration: 300, loss: 1.4700983762741089\n",
      "epoch: 29, iteration: 400, loss: 1.470963716506958\n",
      "epoch: 29, iteration: 500, loss: 1.4663336277008057\n",
      "epoch: 29, iteration: 600, loss: 1.464639663696289\n",
      "epoch: 29, iteration: 700, loss: 1.5384409427642822\n",
      "epoch: 29, iteration: 800, loss: 1.552114725112915\n",
      "epoch: 29, iteration: 900, loss: 1.4770727157592773\n",
      "epoch: 29, iteration: 1000, loss: 1.4656423330307007\n",
      "epoch: 29, iteration: 1100, loss: 1.4649195671081543\n",
      "epoch: 29, iteration: 1200, loss: 1.4666740894317627\n",
      "epoch: 29, iteration: 1300, loss: 1.4946110248565674\n",
      "epoch: 29, iteration: 1400, loss: 1.4642598628997803\n",
      "epoch: 29, iteration: 1500, loss: 1.5283175706863403\n",
      "epoch: 29, iteration: 1600, loss: 1.4735894203186035\n",
      "epoch: 29, iteration: 1700, loss: 1.464050531387329\n",
      "epoch: 29, iteration: 1800, loss: 1.5506254434585571\n",
      "epoch: 29, iteration: 1900, loss: 1.599402904510498\n",
      "epoch: 29, iteration: 2000, loss: 1.5402485132217407\n",
      "epoch: 29, iteration: 2100, loss: 1.462080955505371\n",
      "epoch: 29, iteration: 2200, loss: 1.541373372077942\n",
      "epoch: 29, iteration: 2300, loss: 1.476791501045227\n",
      "epoch: 29, iteration: 2400, loss: 1.4987661838531494\n",
      "epoch: 29, iteration: 2500, loss: 1.5060958862304688\n",
      "epoch: 29, iteration: 2600, loss: 1.4808025360107422\n",
      "epoch: 29, iteration: 2700, loss: 1.4651116132736206\n",
      "epoch: 29, iteration: 2800, loss: 1.4784222841262817\n",
      "epoch: 29, iteration: 2900, loss: 1.4899845123291016\n",
      "epoch: 29, iteration: 3000, loss: 1.5194920301437378\n",
      "epoch: 29, iteration: 3100, loss: 1.4772807359695435\n",
      "epoch: 29, iteration: 3200, loss: 1.4733506441116333\n",
      "epoch: 29, iteration: 3300, loss: 1.5556023120880127\n",
      "epoch: 29, iteration: 3400, loss: 1.4976494312286377\n",
      "epoch: 29, iteration: 3500, loss: 1.471102237701416\n",
      "epoch: 29, iteration: 3600, loss: 1.462104320526123\n",
      "epoch: 29, iteration: 3700, loss: 1.473954439163208\n",
      "epoch: 29,train_accuracy:0.9667, test_accuracy: 0.962\n",
      "epoch: 30, iteration: 0, loss: 1.487323522567749\n",
      "epoch: 30, iteration: 100, loss: 1.5497139692306519\n",
      "epoch: 30, iteration: 200, loss: 1.4705820083618164\n",
      "epoch: 30, iteration: 300, loss: 1.481781244277954\n",
      "epoch: 30, iteration: 400, loss: 1.4656982421875\n",
      "epoch: 30, iteration: 500, loss: 1.4899277687072754\n",
      "epoch: 30, iteration: 600, loss: 1.472540259361267\n",
      "epoch: 30, iteration: 700, loss: 1.4875658750534058\n",
      "epoch: 30, iteration: 800, loss: 1.5248074531555176\n",
      "epoch: 30, iteration: 900, loss: 1.4916603565216064\n",
      "epoch: 30, iteration: 1000, loss: 1.5581512451171875\n",
      "epoch: 30, iteration: 1100, loss: 1.4787876605987549\n",
      "epoch: 30, iteration: 1200, loss: 1.4709171056747437\n",
      "epoch: 30, iteration: 1300, loss: 1.4816899299621582\n",
      "epoch: 30, iteration: 1400, loss: 1.4753222465515137\n",
      "epoch: 30, iteration: 1500, loss: 1.4663366079330444\n",
      "epoch: 30, iteration: 1600, loss: 1.5331377983093262\n",
      "epoch: 30, iteration: 1700, loss: 1.4618299007415771\n",
      "epoch: 30, iteration: 1800, loss: 1.5914238691329956\n",
      "epoch: 30, iteration: 1900, loss: 1.4709844589233398\n",
      "epoch: 30, iteration: 2000, loss: 1.5341824293136597\n",
      "epoch: 30, iteration: 2100, loss: 1.4636549949645996\n",
      "epoch: 30, iteration: 2200, loss: 1.4752693176269531\n",
      "epoch: 30, iteration: 2300, loss: 1.4617985486984253\n",
      "epoch: 30, iteration: 2400, loss: 1.468308925628662\n",
      "epoch: 30, iteration: 2500, loss: 1.5056554079055786\n",
      "epoch: 30, iteration: 2600, loss: 1.4677679538726807\n",
      "epoch: 30, iteration: 2700, loss: 1.4763057231903076\n",
      "epoch: 30, iteration: 2800, loss: 1.4734957218170166\n",
      "epoch: 30, iteration: 2900, loss: 1.4635591506958008\n",
      "epoch: 30, iteration: 3000, loss: 1.5881439447402954\n",
      "epoch: 30, iteration: 3100, loss: 1.4789608716964722\n",
      "epoch: 30, iteration: 3200, loss: 1.4799046516418457\n",
      "epoch: 30, iteration: 3300, loss: 1.5027351379394531\n",
      "epoch: 30, iteration: 3400, loss: 1.5019551515579224\n",
      "epoch: 30, iteration: 3500, loss: 1.4851888418197632\n",
      "epoch: 30, iteration: 3600, loss: 1.5594878196716309\n",
      "epoch: 30, iteration: 3700, loss: 1.4730192422866821\n",
      "epoch: 30,train_accuracy:0.96715, test_accuracy: 0.9615\n",
      "epoch: 31, iteration: 0, loss: 1.4668984413146973\n",
      "epoch: 31, iteration: 100, loss: 1.4708759784698486\n",
      "epoch: 31, iteration: 200, loss: 1.5258049964904785\n",
      "epoch: 31, iteration: 300, loss: 1.475449800491333\n",
      "epoch: 31, iteration: 400, loss: 1.47359037399292\n",
      "epoch: 31, iteration: 500, loss: 1.478430986404419\n",
      "epoch: 31, iteration: 600, loss: 1.468299388885498\n",
      "epoch: 31, iteration: 700, loss: 1.4713765382766724\n",
      "epoch: 31, iteration: 800, loss: 1.4622764587402344\n",
      "epoch: 31, iteration: 900, loss: 1.4654138088226318\n",
      "epoch: 31, iteration: 1000, loss: 1.470992922782898\n",
      "epoch: 31, iteration: 1100, loss: 1.4764972925186157\n",
      "epoch: 31, iteration: 1200, loss: 1.4705995321273804\n",
      "epoch: 31, iteration: 1300, loss: 1.5138261318206787\n",
      "epoch: 31, iteration: 1400, loss: 1.574148416519165\n",
      "epoch: 31, iteration: 1500, loss: 1.4785646200180054\n",
      "epoch: 31, iteration: 1600, loss: 1.4879261255264282\n",
      "epoch: 31, iteration: 1700, loss: 1.4716308116912842\n",
      "epoch: 31, iteration: 1800, loss: 1.4674723148345947\n",
      "epoch: 31, iteration: 1900, loss: 1.6236556768417358\n",
      "epoch: 31, iteration: 2000, loss: 1.4715523719787598\n",
      "epoch: 31, iteration: 2100, loss: 1.4740513563156128\n",
      "epoch: 31, iteration: 2200, loss: 1.5258252620697021\n",
      "epoch: 31, iteration: 2300, loss: 1.4695260524749756\n",
      "epoch: 31, iteration: 2400, loss: 1.500842571258545\n",
      "epoch: 31, iteration: 2500, loss: 1.4988601207733154\n",
      "epoch: 31, iteration: 2600, loss: 1.4723639488220215\n",
      "epoch: 31, iteration: 2700, loss: 1.4786841869354248\n",
      "epoch: 31, iteration: 2800, loss: 1.5407096147537231\n",
      "epoch: 31, iteration: 2900, loss: 1.4892698526382446\n",
      "epoch: 31, iteration: 3000, loss: 1.4719839096069336\n",
      "epoch: 31, iteration: 3100, loss: 1.5255472660064697\n",
      "epoch: 31, iteration: 3200, loss: 1.4824732542037964\n",
      "epoch: 31, iteration: 3300, loss: 1.4723719358444214\n",
      "epoch: 31, iteration: 3400, loss: 1.4737229347229004\n",
      "epoch: 31, iteration: 3500, loss: 1.4698572158813477\n",
      "epoch: 31, iteration: 3600, loss: 1.4689327478408813\n",
      "epoch: 31, iteration: 3700, loss: 1.4955812692642212\n",
      "epoch: 31,train_accuracy:0.9680833333333333, test_accuracy: 0.9624\n",
      "epoch: 32, iteration: 0, loss: 1.483456015586853\n",
      "epoch: 32, iteration: 100, loss: 1.4960858821868896\n",
      "epoch: 32, iteration: 200, loss: 1.4653278589248657\n",
      "epoch: 32, iteration: 300, loss: 1.4628781080245972\n",
      "epoch: 32, iteration: 400, loss: 1.4743432998657227\n",
      "epoch: 32, iteration: 500, loss: 1.4619507789611816\n",
      "epoch: 32, iteration: 600, loss: 1.4926265478134155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32, iteration: 700, loss: 1.4811656475067139\n",
      "epoch: 32, iteration: 800, loss: 1.487295150756836\n",
      "epoch: 32, iteration: 900, loss: 1.461896300315857\n",
      "epoch: 32, iteration: 1000, loss: 1.5326833724975586\n",
      "epoch: 32, iteration: 1100, loss: 1.47844398021698\n",
      "epoch: 32, iteration: 1200, loss: 1.4638805389404297\n",
      "epoch: 32, iteration: 1300, loss: 1.5281447172164917\n",
      "epoch: 32, iteration: 1400, loss: 1.4732990264892578\n",
      "epoch: 32, iteration: 1500, loss: 1.4850465059280396\n",
      "epoch: 32, iteration: 1600, loss: 1.476791501045227\n",
      "epoch: 32, iteration: 1700, loss: 1.4967215061187744\n",
      "epoch: 32, iteration: 1800, loss: 1.469330906867981\n",
      "epoch: 32, iteration: 1900, loss: 1.4641926288604736\n",
      "epoch: 32, iteration: 2000, loss: 1.487766146659851\n",
      "epoch: 32, iteration: 2100, loss: 1.4615529775619507\n",
      "epoch: 32, iteration: 2200, loss: 1.4873542785644531\n",
      "epoch: 32, iteration: 2300, loss: 1.4992022514343262\n",
      "epoch: 32, iteration: 2400, loss: 1.4754763841629028\n",
      "epoch: 32, iteration: 2500, loss: 1.4635601043701172\n",
      "epoch: 32, iteration: 2600, loss: 1.4659435749053955\n",
      "epoch: 32, iteration: 2700, loss: 1.5823454856872559\n",
      "epoch: 32, iteration: 2800, loss: 1.5470881462097168\n",
      "epoch: 32, iteration: 2900, loss: 1.4721479415893555\n",
      "epoch: 32, iteration: 3000, loss: 1.5472992658615112\n",
      "epoch: 32, iteration: 3100, loss: 1.5129263401031494\n",
      "epoch: 32, iteration: 3200, loss: 1.463932752609253\n",
      "epoch: 32, iteration: 3300, loss: 1.4629108905792236\n",
      "epoch: 32, iteration: 3400, loss: 1.4717931747436523\n",
      "epoch: 32, iteration: 3500, loss: 1.4759889841079712\n",
      "epoch: 32, iteration: 3600, loss: 1.4624422788619995\n",
      "epoch: 32, iteration: 3700, loss: 1.5404853820800781\n",
      "epoch: 32,train_accuracy:0.9685, test_accuracy: 0.9623\n",
      "epoch: 33, iteration: 0, loss: 1.4683706760406494\n",
      "epoch: 33, iteration: 100, loss: 1.4707345962524414\n",
      "epoch: 33, iteration: 200, loss: 1.467558741569519\n",
      "epoch: 33, iteration: 300, loss: 1.4827016592025757\n",
      "epoch: 33, iteration: 400, loss: 1.4649800062179565\n",
      "epoch: 33, iteration: 500, loss: 1.5788285732269287\n",
      "epoch: 33, iteration: 600, loss: 1.4661649465560913\n",
      "epoch: 33, iteration: 700, loss: 1.4639755487442017\n",
      "epoch: 33, iteration: 800, loss: 1.519900918006897\n",
      "epoch: 33, iteration: 900, loss: 1.4616379737854004\n",
      "epoch: 33, iteration: 1000, loss: 1.5503190755844116\n",
      "epoch: 33, iteration: 1100, loss: 1.4710241556167603\n",
      "epoch: 33, iteration: 1200, loss: 1.5405558347702026\n",
      "epoch: 33, iteration: 1300, loss: 1.4624576568603516\n",
      "epoch: 33, iteration: 1400, loss: 1.4808447360992432\n",
      "epoch: 33, iteration: 1500, loss: 1.4969685077667236\n",
      "epoch: 33, iteration: 1600, loss: 1.4719539880752563\n",
      "epoch: 33, iteration: 1700, loss: 1.4671881198883057\n",
      "epoch: 33, iteration: 1800, loss: 1.4708647727966309\n",
      "epoch: 33, iteration: 1900, loss: 1.472961664199829\n",
      "epoch: 33, iteration: 2000, loss: 1.4882800579071045\n",
      "epoch: 33, iteration: 2100, loss: 1.4629853963851929\n",
      "epoch: 33, iteration: 2200, loss: 1.507648229598999\n",
      "epoch: 33, iteration: 2300, loss: 1.4811805486679077\n",
      "epoch: 33, iteration: 2400, loss: 1.5601353645324707\n",
      "epoch: 33, iteration: 2500, loss: 1.4871230125427246\n",
      "epoch: 33, iteration: 2600, loss: 1.506649374961853\n",
      "epoch: 33, iteration: 2700, loss: 1.5793681144714355\n",
      "epoch: 33, iteration: 2800, loss: 1.4634777307510376\n",
      "epoch: 33, iteration: 2900, loss: 1.471379280090332\n",
      "epoch: 33, iteration: 3000, loss: 1.4684462547302246\n",
      "epoch: 33, iteration: 3100, loss: 1.46578049659729\n",
      "epoch: 33, iteration: 3200, loss: 1.471215009689331\n",
      "epoch: 33, iteration: 3300, loss: 1.4666650295257568\n",
      "epoch: 33, iteration: 3400, loss: 1.470804214477539\n",
      "epoch: 33, iteration: 3500, loss: 1.4627559185028076\n",
      "epoch: 33, iteration: 3600, loss: 1.538223147392273\n",
      "epoch: 33, iteration: 3700, loss: 1.462976098060608\n",
      "epoch: 33,train_accuracy:0.9689833333333333, test_accuracy: 0.9624\n",
      "epoch: 34, iteration: 0, loss: 1.465786337852478\n",
      "epoch: 34, iteration: 100, loss: 1.6066882610321045\n",
      "epoch: 34, iteration: 200, loss: 1.5592154264450073\n",
      "epoch: 34, iteration: 300, loss: 1.4901199340820312\n",
      "epoch: 34, iteration: 400, loss: 1.5595707893371582\n",
      "epoch: 34, iteration: 500, loss: 1.479048490524292\n",
      "epoch: 34, iteration: 600, loss: 1.4641661643981934\n",
      "epoch: 34, iteration: 700, loss: 1.4614861011505127\n",
      "epoch: 34, iteration: 800, loss: 1.5454645156860352\n",
      "epoch: 34, iteration: 900, loss: 1.4656927585601807\n",
      "epoch: 34, iteration: 1000, loss: 1.4639208316802979\n",
      "epoch: 34, iteration: 1100, loss: 1.4715986251831055\n",
      "epoch: 34, iteration: 1200, loss: 1.4760947227478027\n",
      "epoch: 34, iteration: 1300, loss: 1.4871447086334229\n",
      "epoch: 34, iteration: 1400, loss: 1.4644649028778076\n",
      "epoch: 34, iteration: 1500, loss: 1.5333926677703857\n",
      "epoch: 34, iteration: 1600, loss: 1.4920238256454468\n",
      "epoch: 34, iteration: 1700, loss: 1.4725024700164795\n",
      "epoch: 34, iteration: 1800, loss: 1.4710108041763306\n",
      "epoch: 34, iteration: 1900, loss: 1.4948554039001465\n",
      "epoch: 34, iteration: 2000, loss: 1.4681475162506104\n",
      "epoch: 34, iteration: 2100, loss: 1.4719514846801758\n",
      "epoch: 34, iteration: 2200, loss: 1.4623171091079712\n",
      "epoch: 34, iteration: 2300, loss: 1.4775077104568481\n",
      "epoch: 34, iteration: 2400, loss: 1.4788546562194824\n",
      "epoch: 34, iteration: 2500, loss: 1.4635242223739624\n",
      "epoch: 34, iteration: 2600, loss: 1.4965834617614746\n",
      "epoch: 34, iteration: 2700, loss: 1.47873854637146\n",
      "epoch: 34, iteration: 2800, loss: 1.4701639413833618\n",
      "epoch: 34, iteration: 2900, loss: 1.5364192724227905\n",
      "epoch: 34, iteration: 3000, loss: 1.5322160720825195\n",
      "epoch: 34, iteration: 3100, loss: 1.5186331272125244\n",
      "epoch: 34, iteration: 3200, loss: 1.4683581590652466\n",
      "epoch: 34, iteration: 3300, loss: 1.470824122428894\n",
      "epoch: 34, iteration: 3400, loss: 1.5406131744384766\n",
      "epoch: 34, iteration: 3500, loss: 1.4922711849212646\n",
      "epoch: 34, iteration: 3600, loss: 1.4824374914169312\n",
      "epoch: 34, iteration: 3700, loss: 1.5134422779083252\n",
      "epoch: 34,train_accuracy:0.96955, test_accuracy: 0.9639\n",
      "epoch: 35, iteration: 0, loss: 1.4623394012451172\n",
      "epoch: 35, iteration: 100, loss: 1.4821244478225708\n",
      "epoch: 35, iteration: 200, loss: 1.470008373260498\n",
      "epoch: 35, iteration: 300, loss: 1.4686992168426514\n",
      "epoch: 35, iteration: 400, loss: 1.5300058126449585\n",
      "epoch: 35, iteration: 500, loss: 1.467728614807129\n",
      "epoch: 35, iteration: 600, loss: 1.4846054315567017\n",
      "epoch: 35, iteration: 700, loss: 1.4715135097503662\n",
      "epoch: 35, iteration: 800, loss: 1.4775751829147339\n",
      "epoch: 35, iteration: 900, loss: 1.4626109600067139\n",
      "epoch: 35, iteration: 1000, loss: 1.4926820993423462\n",
      "epoch: 35, iteration: 1100, loss: 1.5272059440612793\n",
      "epoch: 35, iteration: 1200, loss: 1.5159274339675903\n",
      "epoch: 35, iteration: 1300, loss: 1.4867483377456665\n",
      "epoch: 35, iteration: 1400, loss: 1.4690755605697632\n",
      "epoch: 35, iteration: 1500, loss: 1.4642307758331299\n",
      "epoch: 35, iteration: 1600, loss: 1.5389983654022217\n",
      "epoch: 35, iteration: 1700, loss: 1.4629946947097778\n",
      "epoch: 35, iteration: 1800, loss: 1.4662855863571167\n",
      "epoch: 35, iteration: 1900, loss: 1.4947564601898193\n",
      "epoch: 35, iteration: 2000, loss: 1.4721229076385498\n",
      "epoch: 35, iteration: 2100, loss: 1.4895970821380615\n",
      "epoch: 35, iteration: 2200, loss: 1.520376443862915\n",
      "epoch: 35, iteration: 2300, loss: 1.467797040939331\n",
      "epoch: 35, iteration: 2400, loss: 1.4851219654083252\n",
      "epoch: 35, iteration: 2500, loss: 1.4771990776062012\n",
      "epoch: 35, iteration: 2600, loss: 1.461327075958252\n",
      "epoch: 35, iteration: 2700, loss: 1.4659208059310913\n",
      "epoch: 35, iteration: 2800, loss: 1.4783203601837158\n",
      "epoch: 35, iteration: 2900, loss: 1.4857038259506226\n",
      "epoch: 35, iteration: 3000, loss: 1.4801172018051147\n",
      "epoch: 35, iteration: 3100, loss: 1.4624003171920776\n",
      "epoch: 35, iteration: 3200, loss: 1.4769688844680786\n",
      "epoch: 35, iteration: 3300, loss: 1.4749468564987183\n",
      "epoch: 35, iteration: 3400, loss: 1.4674179553985596\n",
      "epoch: 35, iteration: 3500, loss: 1.5142829418182373\n",
      "epoch: 35, iteration: 3600, loss: 1.4779903888702393\n",
      "epoch: 35, iteration: 3700, loss: 1.4684016704559326\n",
      "epoch: 35,train_accuracy:0.97, test_accuracy: 0.9638\n",
      "epoch: 36, iteration: 0, loss: 1.4808681011199951\n",
      "epoch: 36, iteration: 100, loss: 1.5201518535614014\n",
      "epoch: 36, iteration: 200, loss: 1.4687883853912354\n",
      "epoch: 36, iteration: 300, loss: 1.4835625886917114\n",
      "epoch: 36, iteration: 400, loss: 1.4843205213546753\n",
      "epoch: 36, iteration: 500, loss: 1.4886474609375\n",
      "epoch: 36, iteration: 600, loss: 1.4664173126220703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36, iteration: 700, loss: 1.5174899101257324\n",
      "epoch: 36, iteration: 800, loss: 1.464660406112671\n",
      "epoch: 36, iteration: 900, loss: 1.4932210445404053\n",
      "epoch: 36, iteration: 1000, loss: 1.4770299196243286\n",
      "epoch: 36, iteration: 1100, loss: 1.5428338050842285\n",
      "epoch: 36, iteration: 1200, loss: 1.4643266201019287\n",
      "epoch: 36, iteration: 1300, loss: 1.5250177383422852\n",
      "epoch: 36, iteration: 1400, loss: 1.5450881719589233\n",
      "epoch: 36, iteration: 1500, loss: 1.4764599800109863\n",
      "epoch: 36, iteration: 1600, loss: 1.4812986850738525\n",
      "epoch: 36, iteration: 1700, loss: 1.4643943309783936\n",
      "epoch: 36, iteration: 1800, loss: 1.4677797555923462\n",
      "epoch: 36, iteration: 1900, loss: 1.467261552810669\n",
      "epoch: 36, iteration: 2000, loss: 1.5108778476715088\n",
      "epoch: 36, iteration: 2100, loss: 1.468472957611084\n",
      "epoch: 36, iteration: 2200, loss: 1.4688727855682373\n",
      "epoch: 36, iteration: 2300, loss: 1.462707281112671\n",
      "epoch: 36, iteration: 2400, loss: 1.472011685371399\n",
      "epoch: 36, iteration: 2500, loss: 1.4909279346466064\n",
      "epoch: 36, iteration: 2600, loss: 1.562098741531372\n",
      "epoch: 36, iteration: 2700, loss: 1.4799505472183228\n",
      "epoch: 36, iteration: 2800, loss: 1.4801785945892334\n",
      "epoch: 36, iteration: 2900, loss: 1.475414514541626\n",
      "epoch: 36, iteration: 3000, loss: 1.462080478668213\n",
      "epoch: 36, iteration: 3100, loss: 1.4748345613479614\n",
      "epoch: 36, iteration: 3200, loss: 1.463688611984253\n",
      "epoch: 36, iteration: 3300, loss: 1.4682114124298096\n",
      "epoch: 36, iteration: 3400, loss: 1.4748739004135132\n",
      "epoch: 36, iteration: 3500, loss: 1.475029468536377\n",
      "epoch: 36, iteration: 3600, loss: 1.5006152391433716\n",
      "epoch: 36, iteration: 3700, loss: 1.4621973037719727\n",
      "epoch: 36,train_accuracy:0.9705166666666667, test_accuracy: 0.964\n",
      "epoch: 37, iteration: 0, loss: 1.4801117181777954\n",
      "epoch: 37, iteration: 100, loss: 1.4957869052886963\n",
      "epoch: 37, iteration: 200, loss: 1.4803860187530518\n",
      "epoch: 37, iteration: 300, loss: 1.5231592655181885\n",
      "epoch: 37, iteration: 400, loss: 1.5272455215454102\n",
      "epoch: 37, iteration: 500, loss: 1.52338707447052\n",
      "epoch: 37, iteration: 600, loss: 1.4644578695297241\n",
      "epoch: 37, iteration: 700, loss: 1.4683654308319092\n",
      "epoch: 37, iteration: 800, loss: 1.462742805480957\n",
      "epoch: 37, iteration: 900, loss: 1.4637504816055298\n",
      "epoch: 37, iteration: 1000, loss: 1.4668989181518555\n",
      "epoch: 37, iteration: 1100, loss: 1.495345950126648\n",
      "epoch: 37, iteration: 1200, loss: 1.5261143445968628\n",
      "epoch: 37, iteration: 1300, loss: 1.5499815940856934\n",
      "epoch: 37, iteration: 1400, loss: 1.464538335800171\n",
      "epoch: 37, iteration: 1500, loss: 1.5323336124420166\n",
      "epoch: 37, iteration: 1600, loss: 1.4633052349090576\n",
      "epoch: 37, iteration: 1700, loss: 1.475292444229126\n",
      "epoch: 37, iteration: 1800, loss: 1.4751431941986084\n",
      "epoch: 37, iteration: 1900, loss: 1.4794710874557495\n",
      "epoch: 37, iteration: 2000, loss: 1.4616901874542236\n",
      "epoch: 37, iteration: 2100, loss: 1.5360889434814453\n",
      "epoch: 37, iteration: 2200, loss: 1.480110764503479\n",
      "epoch: 37, iteration: 2300, loss: 1.4655241966247559\n",
      "epoch: 37, iteration: 2400, loss: 1.5477505922317505\n",
      "epoch: 37, iteration: 2500, loss: 1.4798238277435303\n",
      "epoch: 37, iteration: 2600, loss: 1.4707401990890503\n",
      "epoch: 37, iteration: 2700, loss: 1.5257865190505981\n",
      "epoch: 37, iteration: 2800, loss: 1.4854083061218262\n",
      "epoch: 37, iteration: 2900, loss: 1.4858664274215698\n",
      "epoch: 37, iteration: 3000, loss: 1.5270761251449585\n",
      "epoch: 37, iteration: 3100, loss: 1.467909574508667\n",
      "epoch: 37, iteration: 3200, loss: 1.4716293811798096\n",
      "epoch: 37, iteration: 3300, loss: 1.5215144157409668\n",
      "epoch: 37, iteration: 3400, loss: 1.4800279140472412\n",
      "epoch: 37, iteration: 3500, loss: 1.47562837600708\n",
      "epoch: 37, iteration: 3600, loss: 1.4747499227523804\n",
      "epoch: 37, iteration: 3700, loss: 1.4646142721176147\n",
      "epoch: 37,train_accuracy:0.9707833333333333, test_accuracy: 0.9647\n",
      "epoch: 38, iteration: 0, loss: 1.4772709608078003\n",
      "epoch: 38, iteration: 100, loss: 1.4713441133499146\n",
      "epoch: 38, iteration: 200, loss: 1.5462219715118408\n",
      "epoch: 38, iteration: 300, loss: 1.465808629989624\n",
      "epoch: 38, iteration: 400, loss: 1.47113835811615\n",
      "epoch: 38, iteration: 500, loss: 1.4838440418243408\n",
      "epoch: 38, iteration: 600, loss: 1.463403582572937\n",
      "epoch: 38, iteration: 700, loss: 1.4731829166412354\n",
      "epoch: 38, iteration: 800, loss: 1.4705862998962402\n",
      "epoch: 38, iteration: 900, loss: 1.4673081636428833\n",
      "epoch: 38, iteration: 1000, loss: 1.5380878448486328\n",
      "epoch: 38, iteration: 1100, loss: 1.4751155376434326\n",
      "epoch: 38, iteration: 1200, loss: 1.4626381397247314\n",
      "epoch: 38, iteration: 1300, loss: 1.470489501953125\n",
      "epoch: 38, iteration: 1400, loss: 1.4892454147338867\n",
      "epoch: 38, iteration: 1500, loss: 1.465211033821106\n",
      "epoch: 38, iteration: 1600, loss: 1.462035059928894\n",
      "epoch: 38, iteration: 1700, loss: 1.5265533924102783\n",
      "epoch: 38, iteration: 1800, loss: 1.4622180461883545\n",
      "epoch: 38, iteration: 1900, loss: 1.462043046951294\n",
      "epoch: 38, iteration: 2000, loss: 1.4719899892807007\n",
      "epoch: 38, iteration: 2100, loss: 1.4695463180541992\n",
      "epoch: 38, iteration: 2200, loss: 1.4628005027770996\n",
      "epoch: 38, iteration: 2300, loss: 1.484821081161499\n",
      "epoch: 38, iteration: 2400, loss: 1.4718016386032104\n",
      "epoch: 38, iteration: 2500, loss: 1.5274133682250977\n",
      "epoch: 38, iteration: 2600, loss: 1.4640144109725952\n",
      "epoch: 38, iteration: 2700, loss: 1.5449705123901367\n",
      "epoch: 38, iteration: 2800, loss: 1.4703892469406128\n",
      "epoch: 38, iteration: 2900, loss: 1.516880989074707\n",
      "epoch: 38, iteration: 3000, loss: 1.4655598402023315\n",
      "epoch: 38, iteration: 3100, loss: 1.4649581909179688\n",
      "epoch: 38, iteration: 3200, loss: 1.462331771850586\n",
      "epoch: 38, iteration: 3300, loss: 1.4831435680389404\n",
      "epoch: 38, iteration: 3400, loss: 1.487993836402893\n",
      "epoch: 38, iteration: 3500, loss: 1.4643734693527222\n",
      "epoch: 38, iteration: 3600, loss: 1.596871018409729\n",
      "epoch: 38, iteration: 3700, loss: 1.4776864051818848\n",
      "epoch: 38,train_accuracy:0.9714333333333334, test_accuracy: 0.965\n",
      "epoch: 39, iteration: 0, loss: 1.4777368307113647\n",
      "epoch: 39, iteration: 100, loss: 1.542224645614624\n",
      "epoch: 39, iteration: 200, loss: 1.462502360343933\n",
      "epoch: 39, iteration: 300, loss: 1.4844640493392944\n",
      "epoch: 39, iteration: 400, loss: 1.4616655111312866\n",
      "epoch: 39, iteration: 500, loss: 1.4614927768707275\n",
      "epoch: 39, iteration: 600, loss: 1.4793578386306763\n",
      "epoch: 39, iteration: 700, loss: 1.472398042678833\n",
      "epoch: 39, iteration: 800, loss: 1.4701902866363525\n",
      "epoch: 39, iteration: 900, loss: 1.4797030687332153\n",
      "epoch: 39, iteration: 1000, loss: 1.4664371013641357\n",
      "epoch: 39, iteration: 1100, loss: 1.4701474905014038\n",
      "epoch: 39, iteration: 1200, loss: 1.4839375019073486\n",
      "epoch: 39, iteration: 1300, loss: 1.4729254245758057\n",
      "epoch: 39, iteration: 1400, loss: 1.5235042572021484\n",
      "epoch: 39, iteration: 1500, loss: 1.5444824695587158\n",
      "epoch: 39, iteration: 1600, loss: 1.4722049236297607\n",
      "epoch: 39, iteration: 1700, loss: 1.4616129398345947\n",
      "epoch: 39, iteration: 1800, loss: 1.4708679914474487\n",
      "epoch: 39, iteration: 1900, loss: 1.5166184902191162\n",
      "epoch: 39, iteration: 2000, loss: 1.4826538562774658\n",
      "epoch: 39, iteration: 2100, loss: 1.4743502140045166\n",
      "epoch: 39, iteration: 2200, loss: 1.4815220832824707\n",
      "epoch: 39, iteration: 2300, loss: 1.519627571105957\n",
      "epoch: 39, iteration: 2400, loss: 1.4757145643234253\n",
      "epoch: 39, iteration: 2500, loss: 1.461324691772461\n",
      "epoch: 39, iteration: 2600, loss: 1.461933970451355\n",
      "epoch: 39, iteration: 2700, loss: 1.4706143140792847\n",
      "epoch: 39, iteration: 2800, loss: 1.4690662622451782\n",
      "epoch: 39, iteration: 2900, loss: 1.5175786018371582\n",
      "epoch: 39, iteration: 3000, loss: 1.4627048969268799\n",
      "epoch: 39, iteration: 3100, loss: 1.469605565071106\n",
      "epoch: 39, iteration: 3200, loss: 1.4659342765808105\n",
      "epoch: 39, iteration: 3300, loss: 1.4619343280792236\n",
      "epoch: 39, iteration: 3400, loss: 1.461604356765747\n",
      "epoch: 39, iteration: 3500, loss: 1.4682363271713257\n",
      "epoch: 39, iteration: 3600, loss: 1.4628162384033203\n",
      "epoch: 39, iteration: 3700, loss: 1.493227243423462\n",
      "epoch: 39,train_accuracy:0.97195, test_accuracy: 0.9653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n print(\\'=========================================\\')\\n    print(\"Model\\'s state_dict for epoch :\", i_epoch)\\n    for param_tensor in model.state_dict():\\n     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\\n\\n    print(\"Optimizer\\'s state_dict:\")\\n     for var_name in optimizer.state_dict():\\n      print(var_name, \"\\t\", optimizer.state_dict()[var_name])\\n\\n    print(\\'=========================================\\')\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\"\"\"\n",
    "TODO: 학습횟수를 자유자재로 바꾸어보자.\n",
    "\"\"\"\n",
    "\n",
    "for i_epoch in range(40):\n",
    "    train(model, train_loader, optimizer, i_epoch, device)\n",
    "    test(model, test_loader, i_epoch, device)\n",
    "    torch.save(model.state_dict(), \"mnist_test.pt\")\n",
    "\n",
    "\"\"\"\n",
    " print('=========================================')\n",
    "    print(\"Model's state_dict for epoch :\", i_epoch)\n",
    "    for param_tensor in model.state_dict():\n",
    "     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "    print(\"Optimizer's state_dict:\")\n",
    "     for var_name in optimizer.state_dict():\n",
    "      print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "\n",
    "    print('=========================================')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23c93b16610>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#그래프 확인\n",
    "plt.plot(loss_value, label='Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23c909a5670>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_accuracy_value, label='Train Accuracy', c = 'b')\n",
    "plt.plot(test_accuracy_value, label='Test Accuracy', c= 'r')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_accuracy_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9653\n"
     ]
    }
   ],
   "source": [
    "print(test_accuracy_value[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
